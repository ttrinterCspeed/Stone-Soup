{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDP - Stone Soup Experiments\n",
    "[Documentation](https://stonesoup.readthedocs.io/en/v1.4/auto_examples/readers/Custom_Pandas_Dataloader.html#sphx-glr-auto-examples-readers-custom-pandas-dataloader-py)\n",
    "\n",
    "Continuing experiments - following tutorial #6: Data association - multi-target tracking tutorial.\n",
    "\n",
    "For this tutorial, I need to extend the test data set from the previous notebook to add a second target. I'll look for a target of opportunity that is observed at the same time as the target considered so far. If possible, a few different cases would be interesting:\n",
    "1. A straight flight that does not interesect the first flight path\n",
    "1. A straight flight that DOES intersect the first flight path, but at a different time: 10610889\n",
    "1. A non-linear flight that does not cross the first flight\n",
    "1. A non-linear flight that DOES cross the first flight path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from importlib import reload  # Python 3.4+\n",
    "from typing import Tuple\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "from math import ceil\n",
    "from ordered_set import OrderedSet\n",
    "\n",
    "import dateutil\n",
    "from pymap3d import geodetic2enu\n",
    "\n",
    "import sys\n",
    "sys.path.append('C:/Users/ttrinter/git_repo/cspeed/data_common')\n",
    "sys.path.append('../../..')\n",
    "import data_functions as dfunc\n",
    "import visualizations as v\n",
    "from ttt_ss_funcs import generate_timestamps, ADSBTruthReader, CSVReaderXY, CSVReaderPolar, plot_all, CSVClutterReaderXY, group_plots\n",
    "\n",
    "\n",
    "from stonesoup.reader import DetectionReader, GroundTruthReader\n",
    "from stonesoup.base import Property\n",
    "from stonesoup.types.detection import Detection, Clutter\n",
    "from stonesoup.plotter import AnimatedPlotterly, Plotter\n",
    "\n",
    "from stonesoup.base import Property\n",
    "from stonesoup.buffered_generator import BufferedGenerator\n",
    "from stonesoup.functions import cart2sphere, sphere2cart\n",
    "from stonesoup.models.measurement.linear import LinearGaussian\n",
    "from stonesoup.models.measurement.nonlinear import CartesianToBearingRange\n",
    "from stonesoup.types.angle import Bearing\n",
    "from stonesoup.types.detection import Detection\n",
    "from stonesoup.types.groundtruth import GroundTruthState, GroundTruthPath\n",
    "\n",
    "# Tracker Imports\n",
    "from stonesoup.types.state import GaussianState\n",
    "\n",
    "plot_type = 'static' # or 'animated'\n",
    "\n",
    "sensor_positions = { 'RDU103': (51.52126391, 5.85862734)}\n",
    "\n",
    "METERS_in_NM = 1852\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data from BigQuery\n",
    "* rdp_straight: short, straight flight path\n",
    "* rdp_extended: longer flight path\n",
    "* adsb_straight: truth for rdp_straight\n",
    "* adsb_extended: truth for rdp_extended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_address1 = 10537421 # plane #1\n",
    "target_address2 = 10610889 # plane #\n",
    "# adsb_sql = f\"\"\"SELECT `timestamp`,\n",
    "#         time_of_day, \n",
    "#         latitude, \n",
    "#         longitude, \n",
    "#         target_address,\n",
    "#         flight_level, \n",
    "#         rho, \n",
    "#         theta\n",
    "# FROM radar_data.adsb\n",
    "# WHERE test_date = '2024-07-17'\n",
    "# AND target_address={target_address}\n",
    "# and latitude is not NULL\n",
    "# AND rho<20\n",
    "# ORDER BY `timestamp`\"\"\"\n",
    "\n",
    "# adsb = dfunc.query_to_df(adsb_sql)\n",
    "# data_dir = 'C:/Users/ttrinter/git_repo/Stone-Soup/data'\n",
    "# adsb_file = f'{data_dir}/adsb_straight2.csv'\n",
    "# adsb.to_csv(adsb_file, index=False)\n",
    "\n",
    "# rdp_sql = f\"\"\"SELECT \n",
    "#         `timestamp`,\n",
    "#         time_of_day,\n",
    "#         cal, \n",
    "#         rho,\n",
    "#         theta, \n",
    "#         x, \n",
    "#         y, \n",
    "#         field_note \n",
    "# FROM radar_data.rdp\n",
    "\n",
    "# --Clutter Where Statement\n",
    "# WHERE test_date = '2024-07-17'\n",
    "# AND sortie_id=61\n",
    "# AND `timestamp` >= '{start_time.strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "# AND `timestamp` <= '{end_time.strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "# ORDER BY `timestamp`\"\"\"\n",
    "\n",
    "# # # Test Plane Where\n",
    "# # # WHERE `timestamp` >= '{adsb_straight.timestamp.min().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "# # # AND `timestamp` <= '{adsb_straight.timestamp.max().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "# # # AND rho >= {adsb_straight.rho.min()-0.5}\n",
    "# # # AND rho <= {adsb_straight.rho.max()+0.5}\n",
    "# # # AND theta >= {adsb_straight.theta.min()- 5}\n",
    "# # # AND theta <= {adsb_straight.theta.max()+5}\"\"\"\n",
    "\n",
    "# # # # # # rdp_straight = dfunc.query_to_df(rdp_sql)\n",
    "# # # # # # rdp_straight.head()\n",
    "\n",
    "# rdp_clutter = dfunc.query_to_df(rdp_sql)\n",
    "# rdp_clutter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass Number\n",
    "All of the examples are looking at data once/second. That matches up the frequency of new plots. However, for our purposes, it may be better to look per radar pass, assuming that the radar will not see a target more than once per pass.\n",
    "\n",
    "I should be able to set the pass number on the plots and clutter using the cat 34 sector messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_sql = \"\"\"SELECT * \n",
    "#             FROM radar_data.positions \n",
    "#             WHERE sortie_id=61 \n",
    "#             ORDER BY `timestamp`\"\"\"\n",
    "\n",
    "# pos_data = dfunc.query_to_df(pos_sql)\n",
    "# pos_data['timestamp'] = pos_data['timestamp'].dt.tz_localize(None)\n",
    "# pos_data['timestamp'] = pos_data['timestamp'].astype('datetime64[us]')\n",
    "# pos_data['timestamp'] = pos_data['timestamp'].round('ms')\n",
    "# pos_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather Clutter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove timezone from timestamps\n",
    "# rdp_clutter['timestamp'] = rdp_clutter['timestamp'].dt.tz_localize(None)\n",
    "# rdp_clutter['timestamp'] = rdp_clutter['timestamp'].astype('datetime64[us]')\n",
    "# rdp_clutter['timestamp'] = rdp_clutter['timestamp'].round('ms')\n",
    "\n",
    "# # add pass number - may use that for tracker iterations\n",
    "# rdp_clutter = dfunc.add_pass_no(rdp_clutter, pos_data)\n",
    "\n",
    "# all_matched_data['timestamp'] = all_matched_data['timestamp_rdp'].dt.tz_localize(None)\n",
    "# all_matched_data['timestamp'] = all_matched_data['timestamp'].astype('datetime64[us]')\n",
    "# all_matched_data['timestamp'] = all_matched_data['timestamp'].round('ms')\n",
    "\n",
    "# # # Select only needed columns\n",
    "# cols_to_keep = ['timestamp','rho','theta','cal','x','y', 'pass_no']\n",
    "# rdp_clutter = rdp_clutter[cols_to_keep]\n",
    "\n",
    "# # Left JOIN with the matched data to eliminate all RDP plots matched to any targets of opportunity\n",
    "# rdp_clutter = pd.merge(rdp_clutter, all_matched_data.loc[all_matched_data.close_enough==1, ['rho_rdp','theta_rdp','timestamp']], \n",
    "#                                left_on=['rho','theta','timestamp'], \n",
    "#                                right_on = ['rho_rdp','theta_rdp','timestamp'], \n",
    "#                                how='left')\n",
    "\n",
    "# # Drop rows that matched a target of opportunity\n",
    "# rdp_clutter = rdp_clutter.loc[rdp_clutter.rho_rdp.isna()]\n",
    "# # filter for only the range where the test plane is to simplify things\n",
    "# rdp_clutter = rdp_clutter.loc[(rdp_clutter.rho>=matched_data.rho_rdp.min()) &\n",
    "#                               (rdp_clutter.rho<=matched_data.rho_rdp.max())&\n",
    "#                               (rdp_clutter.theta>=matched_data.theta_rdp.min()) &\n",
    "#                               (rdp_clutter.theta<=matched_data.theta_rdp.max()) & \n",
    "#                               (rdp_clutter['timestamp'] >= matched_data.timestamp_rdp.min()) &\n",
    "#                               (rdp_clutter['timestamp'] <= matched_data.timestamp_rdp.max())]\n",
    "\n",
    "# rdp_clutter['theta_rad'] = np.deg2rad(rdp_clutter.theta)\n",
    "\n",
    "# data_dir = 'C:/Users/ttrinter/git_repo/Stone-Soup/data'\n",
    "# clutter_filename = f'{data_dir}/sample_clutter.csv'\n",
    "# rdp_clutter.to_csv(clutter_filename, index=False)\n",
    "# len(rdp_clutter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/Read  to/from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "data_dir = 'C:/Users/ttrinter/git_repo/Stone-Soup/data'\n",
    "\n",
    "# Now there are 2 ADSB Files\n",
    "adsb_data = pd.DataFrame()\n",
    "adsb_files = [f'{data_dir}/adsb_straight.csv', f'{data_dir}/adsb_straight2.csv']\n",
    "for file in adsb_files:\n",
    "    this_data = pd.read_csv(file)\n",
    "    this_data['timestamp'] = pd.to_datetime(this_data['timestamp'], errors='coerce')\n",
    "    this_data = this_data.loc[~this_data['timestamp'].isna()]\n",
    "    this_data['timestamp'] = pd.to_datetime(this_data['timestamp'], errors='coerce')\n",
    "    this_data['timestamp'] = this_data['timestamp'].dt.tz_localize(None)\n",
    "\n",
    "    adsb_data = pd.concat([adsb_data, this_data])\n",
    "\n",
    "# ADSB- first 3 min\n",
    "# adsb_file = f'{data_dir}/adsb3.csv'\n",
    "# adsb_data = pd.read_csv(adsb_file)\n",
    "\n",
    "rdp_file = f'{data_dir}/rdp_straight.csv'\n",
    "# rdp_straight['timestamp'] = pd.to_datetime(rdp_straight['timestamp'], errors='coerce')\n",
    "# rdp_straight['theta_rad'] = np.deg2rad(rdp_straight.theta)\n",
    "# rdp_straight.loc[rdp_straight.theta_rad>2*pi, 'theta_rad'] = rdp_straight.loc[rdp_straight.theta_rad>2*pi, 'theta_rad'] - 2*pi \n",
    "\n",
    "# rdp_straight = rdp_straight.loc[~rdp_straight['timestamp'].isna()]\n",
    "# rdp_straight.to_csv(rdp_file, index=False)\n",
    "# rdp_data = pd.read_csv(rdp_file)\n",
    "# rdp_data['timestamp'] = pd.to_datetime(rdp_data['timestamp'], errors='coerce')\n",
    "# rdp_data['timestamp'] = rdp_data['timestamp'].dt.tz_localize(None)\n",
    "# rdp_data['timestamp'] = rdp_data['timestamp'].astype('datetime64[us]')\n",
    "# rdp_data['timestamp'] = rdp_data['timestamp'].round('ms')\n",
    "\n",
    "\n",
    "# Matched Plots\n",
    "matched_csv1 = f'{data_dir}/rdp_matched.csv'\n",
    "matched_csv2 = f'{data_dir}/rdp_matched2.csv'\n",
    "matched_csv3min = f'{data_dir}/rdp_matched3.csv'\n",
    "matched_rdp_file = matched_csv2\n",
    "\n",
    "rdp_matched = pd.read_csv(matched_rdp_file)\n",
    "rdp_matched['timestamp'] = pd.to_datetime(rdp_matched['timestamp'], errors='coerce')\n",
    "rdp_matched['timestamp'] = rdp_matched['timestamp'].dt.tz_localize(None)\n",
    "rdp_matched['timestamp'] = rdp_matched['timestamp'].astype('datetime64[us]')\n",
    "rdp_matched['timestamp'] = rdp_matched['timestamp'].round('ms')\n",
    "\n",
    "start_time = rdp_matched['timestamp'].min()\n",
    "end_time = rdp_matched['timestamp'].max()\n",
    "\n",
    "clutter_file = f'{data_dir}/sample_clutter.csv'\n",
    "# clutter_file = f'{data_dir}/clutter3.csv'\n",
    "clutter_data = pd.read_csv(clutter_file)\n",
    "clutter_data['timestamp'] = pd.to_datetime(clutter_data['timestamp'], errors='coerce')\n",
    "clutter_data = clutter_data.loc[~clutter_data['timestamp'].isna()]\n",
    "clutter_data['timestamp'] = pd.to_datetime(clutter_data['timestamp'], errors='coerce')\n",
    "clutter_data['timestamp'] = clutter_data['timestamp'].dt.tz_localize(None)\n",
    "\n",
    "print(f'ADSB: {len(adsb_data)}')\n",
    "# print(f'RDP: {len(rdp_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that there are no clutter points identical to matched points - I've already tried to remove all of these, but still see some dupes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdp_matched['ts_10ms'] = [x.round('10ms') for x in rdp_matched['timestamp']]\n",
    "clutter_data['ts_10ms'] = [x.round('10ms') for x in clutter_data['timestamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_clutter = clutter_data.merge(rdp_matched, left_on=['ts_10ms','rho','theta'], right_on=['ts_10ms','rho','theta'], how='left', suffixes=[\"\", 'rdp_match'])\n",
    "matched_clutter.loc[matched_clutter.xrdp_match.isna(), clutter_data.columns].to_csv(clutter_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shorter Files\n",
    "The tracker is missing a lot of the early plots in the track for some reason. So I'll cut down the test data to the first 3 minutes to see how it does with that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_start = rdp_matched.timestamp.min()\n",
    "# sub_end = sub_start + timedelta(minutes=3)\n",
    "\n",
    "# matched_rdp3 = rdp_matched.loc[(rdp_matched.timestamp>=sub_start) & (rdp_matched.timestamp<=sub_end)]\n",
    "# matched_csv3min = f'{data_dir}/rdp_matched3.csv'\n",
    "# matched_rdp3.to_csv(matched_csv3min, index=False)\n",
    "\n",
    "# adsb3 = adsb_data.loc[(adsb_data.timestamp>=sub_start) & (adsb_data.timestamp<=sub_end)]\n",
    "# adsb_csv3min = f'{data_dir}/adsb3.csv'\n",
    "# adsb3.to_csv(adsb_csv3min, index=False)\n",
    "\n",
    "# clutter3 = clutter_data.loc[(clutter_data.timestamp>=sub_start) & (clutter_data.timestamp<=sub_end)]\n",
    "# clutter_csv3min = f'{data_dir}/clutter3.csv'\n",
    "# clutter3.to_csv(clutter_csv3min, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdp_data = pd.read_csv(rdp_file)\n",
    "# rdp_data['timestamp'] = pd.to_datetime(rdp_data['timestamp'], errors='coerce')\n",
    "# rdp_data['timestamp'] = rdp_data['timestamp'].dt.tz_localize(None)\n",
    "# rdp_data['timestamp'] = rdp_data['timestamp'].astype('datetime64[us]')\n",
    "# rdp_data['timestamp'] = rdp_data['timestamp'].round('ms')\n",
    "\n",
    "# rdp_data = rdp_data.loc[~rdp_data['timestamp'].isna()]\n",
    "# rdp_data = dfunc.add_pass_no(rdp_data, pos_data)\n",
    "# rdp_data.pass_no.value_counts().sort_index()\n",
    "\n",
    "# rdp_data.to_csv(rdp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdp_matched = pd.read_csv(matched_csv1)\n",
    "# rdp_matched['timestamp'] = pd.to_datetime(rdp_matched['timestamp'], errors='coerce')\n",
    "# rdp_matched['timestamp'] = rdp_matched['timestamp'].dt.tz_localize(None)\n",
    "# rdp_matched['timestamp'] = rdp_matched['timestamp'].astype('datetime64[us]')\n",
    "# rdp_matched['timestamp'] = rdp_matched['timestamp'].round('ms')\n",
    "\n",
    "# rdp_matched = dfunc.add_pass_no(rdp_matched, pos_data)\n",
    "# # rdp_matched.pass_no.value_counts().sort_index()\n",
    "\n",
    "# rdp_matched.to_csv(matched_csv1, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{clutter_data.timestamp.min()} : {clutter_data.timestamp.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.scatter_targets(clutter_data)\n",
    "plt.suptitle('Clutter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matched Data Set\n",
    "To make things even simpler, I'll grab the set of matched data for this test plane. Then most of the plots should be \"true\" detections. Let's see how the tracker does with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_address1 = 10537421 # plane #1\n",
    "target_address2 = 10610889 # plane #\n",
    "target_addresses = [target_address1, target_address2]\n",
    "file_dir = 'C:/Users/ttrinter/OneDrive - cspeed.com (1)/Documents/Data/Travis/2024-07-17'\n",
    "matched_file = '20240717_Travis_matched_rdp_61.xlsx'\n",
    "all_matched_data = pd.read_excel(f'{file_dir}/{matched_file}')\n",
    "matched_data = all_matched_data.loc[(all_matched_data.target_address.isin(target_addresses)) &\n",
    "                                (all_matched_data.close_enough==True) & \n",
    "                                (all_matched_data.timestamp_adsb>=start_time) & \n",
    "                                (all_matched_data.timestamp_adsb<=end_time)]\n",
    "matched_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_plot = v.plot_target_match2(matching=matched_data, \n",
    "                                    target_address=target_address1, \n",
    "                                    plot_show=True, \n",
    "                                    pd_loc='title')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_plot = v.plot_target_match2(matching=matched_data, \n",
    "                                    target_address=target_address2, \n",
    "                                    plot_show=True, \n",
    "                                    pd_loc='title')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdp_matched = matched_data[['timestamp_rdp',\n",
    "#                             'cal_rdp',\n",
    "#                             'rho_rdp',\n",
    "#                             'theta_rdp', \n",
    "#                             'target_address']]\n",
    "\n",
    "# rdp_matched['theta_rad'] = np.deg2rad(rdp_matched.theta_rdp)\n",
    "# # rdp_matched.loc[rdp_matched.theta_rad>2*pi, 'theta_rad'] = rdp_straight.loc[rdp_straight.theta_rad>2*pi, 'theta_rad'] - 2*pi \n",
    "# rdp_matched.rename(columns={'rho_rdp': 'rho',\n",
    "#                             'theta_rdp': 'theta', \n",
    "#                             'timestamp_rdp': 'timestamp', \n",
    "#                             'cal_rdp': 'cal'}, \n",
    "#                             inplace=True)\n",
    "\n",
    "# rdp_matched['x'], rdp_matched['y'] = zip(*rdp_matched.apply(lambda x: dfunc.polar_to_cartesian(x.rho, x.theta), axis=1))\n",
    "\n",
    "# matched_csv2 = f'{data_dir}/rdp_matched2.csv'\n",
    "# rdp_matched.to_csv(matched_csv2, index=False)\n",
    "\n",
    "# rdp_matched.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Group by the 'category' column and plot each group separately\n",
    "for tgt, group in rdp_matched.groupby('target_address'):\n",
    "    ax.scatter(group['x'], group['y'], label=tgt, marker=\"+\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Matched RDP Plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdp_matched['x_m'] = rdp_matched.x * METERS_in_NM\n",
    "rdp_matched['y_m'] = rdp_matched.y * METERS_in_NM\n",
    "rdp_matched.sort_values('timestamp', inplace=True)\n",
    "\n",
    "# Track-starts:\n",
    "track_start1_t = rdp_matched.loc[rdp_matched.target_address==target_address1].iloc[0]['timestamp']\n",
    "track_start1_x = rdp_matched.loc[rdp_matched.target_address==target_address1].iloc[0]['x_m']\n",
    "track_start1_y = rdp_matched.loc[rdp_matched.target_address==target_address1].iloc[0]['y_m']\n",
    "\n",
    "track_start2_t = rdp_matched.loc[rdp_matched.target_address==target_address2].iloc[0]['timestamp']\n",
    "track_start2_x = rdp_matched.loc[rdp_matched.target_address==target_address2].iloc[0]['x_m']\n",
    "track_start2_y = rdp_matched.loc[rdp_matched.target_address==target_address2].iloc[0]['y_m']\n",
    "\n",
    "rdp_matched[['target_address','timestamp', 'rho','theta','x_m','y_m']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adsb = ADSBTruthReader.multiple_ground_truth_reader(adsb_files)\n",
    "# adsb = ADSBTruthReader.single_ground_truth_reader(adsb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = Plotter()\n",
    "plotter.plot_ground_truths(adsb, \n",
    "                        mapping=[0, 2], \n",
    "                        markersize = 5,\n",
    "                        marker = 's', \n",
    "                        markerfacecolor = 'none', \n",
    "                        alpha = 0.2)\n",
    "plt.grid()\n",
    "plt.title(\"Two Targets of Opportunity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detections\n",
    "matched_xy = CSVReaderXY(matched_rdp_file)\n",
    "matched_polar = CSVReaderPolar(matched_rdp_file)\n",
    "\n",
    "# Cutter\n",
    "clutter = CSVClutterReaderXY(clutter_file)\n",
    "\n",
    "dets = [next(iter(detection[1])) for detection in matched_xy.detections_gen()]\n",
    "cluts = [next(iter(detection[1])) for detection in clutter.detections_gen()]\n",
    "\n",
    "# Combine detections with clutter\n",
    "all_measurements = dets + cluts\n",
    "all_measurements.sort(key=lambda obj: obj.timestamp)\n",
    "\n",
    "plot_all(start_time, \n",
    "         end_time,\n",
    "         all_measurements=all_measurements, \n",
    "         adsb=adsb, \n",
    "         plot_type='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Tutorial #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stonesoup.models.transition.linear import CombinedLinearGaussianTransitionModel, \\\n",
    "                                               ConstantVelocity\n",
    "from stonesoup.predictor.kalman import KalmanPredictor\n",
    "from stonesoup.updater.kalman import KalmanUpdater\n",
    "from stonesoup.predictor.kalman import UnscentedKalmanPredictor\n",
    "from stonesoup.updater.kalman import UnscentedKalmanUpdater\n",
    "\n",
    "from stonesoup.types.track import Track\n",
    "from stonesoup.hypothesiser.distance import DistanceHypothesiser\n",
    "from stonesoup.measures import Mahalanobis, Euclidean\n",
    "\n",
    "from stonesoup.dataassociator.neighbour import NearestNeighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measurement Model\n",
    "np.random.seed(42)\n",
    "default_variance = 50\n",
    "\n",
    "measurement_model = LinearGaussian(\n",
    "    ndim_state=4,   # Number of state dimensions (position and velocity in 2D)\n",
    "    mapping=(0, 2), # Mapping measurement vector index to state index\n",
    "    noise_covar=np.array([[default_variance, 0 ],  \n",
    "                          [0, default_variance]])\n",
    "    )  #Covariance matrix for Gaussian PDF\n",
    "\n",
    "# Transition Model\n",
    "q_const = 60\n",
    "q_x = q_const\n",
    "q_y = q_const\n",
    "transition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(q_x),\n",
    "                                                          ConstantVelocity(q_y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = KalmanPredictor(transition_model)\n",
    "updater = KalmanUpdater(measurement_model)\n",
    "\n",
    "# predictor = UnscentedKalmanPredictor(transition_model)\n",
    "# updater = UnscentedKalmanUpdater(measurement_model)  # Keep alpha as default = 0.5\n",
    "\n",
    "hypothesiser = DistanceHypothesiser(predictor, updater, measure=Mahalanobis(), missed_distance=3)\n",
    "# hypothesiser = DistanceHypothesiser(predictor, updater, measure=Euclidean(), missed_distance=80)\n",
    "data_associator = NearestNeighbour(hypothesiser)\n",
    "\n",
    "# Clear things out from prior runs\n",
    "hypothesis = None\n",
    "post = None\n",
    "\n",
    "prior1 = GaussianState([[track_start1_x], [1], [track_start1_y], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
    "prior2 = GaussianState([[track_start2_x], [1], [track_start2_y], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
    "\n",
    "# create a prior using the location of the radar\n",
    "# prior = GaussianState([[0], [q_const], [0], [q_const]], np.diag([default_variance, 0.5, default_variance, 0.5]), timestamp=start_time)\n",
    "\n",
    "# Loop through the predict, hypothesise, associate and update steps.\n",
    "# track = Track([prior1])\n",
    "if \"tracks\" in globals():\n",
    "    del tracks\n",
    "\n",
    "if \"track\" in globals():\n",
    "    del(track)\n",
    "\n",
    "tracks = {Track([prior1]), Track([prior2])}\n",
    "\n",
    "grouped_sec, grouped_pass = group_plots(all_measurements)\n",
    "\n",
    "# for n, measurements in enumerate(grouped_sec):\n",
    "for n, measurements in enumerate(grouped_pass):    \n",
    "    this_time = min(measurements, key=lambda meas: meas.timestamp).timestamp\n",
    "    this_time = this_time.replace(microsecond=0)\n",
    "\n",
    "    # Calculate all hypothesis pairs and associate the elements in the best subset to the tracks.\n",
    "    if len(measurements)>0:\n",
    "    # for n, measurements in enumerate(dets):\n",
    "        try: \n",
    "            hypotheses = data_associator.associate(tracks,\n",
    "                                                measurements,\n",
    "                                                this_time)\n",
    "            for track in tracks:\n",
    "                hypothesis = hypotheses[track]\n",
    "                if hypothesis.measurement:\n",
    "                    post = updater.update(hypothesis)\n",
    "                    track.append(post)\n",
    "                else:  # When data associator says no detections are good enough, we'll keep the prediction\n",
    "                    track.append(hypothesis.prediction)\n",
    "\n",
    "        except:\n",
    "            # print(f'{this_time}: {len(measurements)}: ERROR')\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the ground truth and measurements with clutter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(start_time,\n",
    "         end_time,\n",
    "         all_measurements=all_measurements, \n",
    "         adsb=adsb,\n",
    "         tracks=tracks,\n",
    "         plot_type='static')\n",
    "        # plot_type='animated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tracker did a good job of following each of the tracks separately! However, the more vertical track was wierdly combined with what appears to be an entirely different track in another location and direction.\n",
    "\n",
    "Similar to the previous analysis - after running the polar tracks, re-running these cartesian tracks no longer works! Clearly something is lingering after a run. Not all inputs are getting reset with a re-run. Need to sort that out!\n",
    "\n",
    "It is also suspect that 14 correlated plots get passed and are not added to the first track. It is not clear what/why these plots are skipped. Need to investigate that too. Perhaps with a smaller dataset that only inlcudes the first 20 correlated plots or something.\n",
    "\n",
    "Maybe the subsequent tutorials will address track deletions and initiation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polar Coordinates\n",
    "Trying again, but changing the process to read rho and theta and, maybe later also radial velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detections\n",
    "meas_polar = CSVReaderPolar(matched_csv2)\n",
    "\n",
    "dets = [next(iter(detection[1])) for detection in meas_polar.detections_gen()]\n",
    "cluts = [next(iter(detection[1])) for detection in clutter.detections_gen()]\n",
    "\n",
    "# Combine detections with clutter\n",
    "all_measurements = dets + cluts\n",
    "all_measurements.sort(key=lambda obj: obj.timestamp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kalman Filtering Again...\n",
    "Should be the same from here forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition Model\n",
    "q_const = 35\n",
    "q_x = q_const\n",
    "q_y = q_const\n",
    "default_variance=50\n",
    "\n",
    "transition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(q_x),\n",
    "                                                          ConstantVelocity(q_y)])\n",
    "\n",
    "# measurement_model = CartesianToBearingRange(ndim_state=4, \n",
    "#                                             mapping=(0,2), \n",
    "#                                             noise_covar=np.array([[default_variance, 0 ],\n",
    "#                                                                   [0, default_variance]]))\n",
    "\n",
    "# Create prior\n",
    "# predictor = KalmanPredictor(transition_model)\n",
    "# updater = KalmanUpdater(measurement_model)\n",
    "\n",
    "predictor = UnscentedKalmanPredictor(transition_model)\n",
    "updater = UnscentedKalmanUpdater(measurement_model)  # Keep alpha as default = 0.5\n",
    "\n",
    "hypothesiser = DistanceHypothesiser(predictor, updater, measure=Mahalanobis(), missed_distance=3)\n",
    "data_associator = NearestNeighbour(hypothesiser)\n",
    "\n",
    "# create a prior using the approximate start of the track\n",
    "# prior = GaussianState([[track_start_x], [1], [track_start_y], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
    "\n",
    "prior1 = GaussianState([[track_start1_x], [1], [track_start1_y], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
    "prior2 = GaussianState([[track_start2_x], [1], [track_start2_y], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
    "\n",
    "# create a prior using the location of the radar\n",
    "# prior = GaussianState([[0], [q_const], [0], [q_const]], np.diag([default_variance, 0.5, default_variance, 0.5]), timestamp=start_time)\n",
    "\n",
    "# Loop through the predict, hypothesise, associate and update steps.\n",
    "# del(tracks)\n",
    "if \"tracks\" in globals():\n",
    "    del tracks\n",
    "\n",
    "if \"track\" in globals():\n",
    "    del(track)\n",
    "\n",
    "tracks = {Track([prior1]), Track([prior2])}\n",
    "\n",
    "grouped_sec, grouped_pass = group_plots(all_measurements)\n",
    "\n",
    "# for n, measurements in enumerate(grouped_sec):\n",
    "for n, measurements in enumerate(grouped_pass):    \n",
    "    this_time = min(measurements, key=lambda meas: meas.timestamp).timestamp\n",
    "    this_time = this_time.replace(microsecond=0)\n",
    "\n",
    "    # Calculate all hypothesis pairs and associate the elements in the best subset to the tracks.\n",
    "    if len(measurements)>0:\n",
    "    # for n, measurements in enumerate(dets):\n",
    "        try: \n",
    "            hypotheses = data_associator.associate(tracks,\n",
    "                                                measurements,\n",
    "                                                this_time)\n",
    "            for track in tracks:\n",
    "                hypothesis = hypotheses[track]\n",
    "                if hypothesis.measurement:\n",
    "                    post = updater.update(hypothesis)\n",
    "                    track.append(post)\n",
    "                else:  # When data associator says no detections are good enough, we'll keep the prediction\n",
    "                    track.append(hypothesis.prediction)\n",
    "\n",
    "        except:\n",
    "            # print(f'{this_time}: {len(measurements)}: ERROR')\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(start_time,\n",
    "         end_time,\n",
    "         all_measurements=all_measurements, \n",
    "         adsb=adsb,\n",
    "         tracks=tracks,\n",
    "         plot_type='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is unclear why the cartesian tracker worked reasonably well (besides continuing the track with a large change in direction and location) and the polar tracker didn't ever continue the track.\n",
    "\n",
    "Leaving it for the next tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grouped_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cspeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
