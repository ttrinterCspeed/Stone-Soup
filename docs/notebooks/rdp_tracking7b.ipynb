{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDP - Stone Soup #7: PDA - (modified from #5)\n",
    "[Documentation](https://stonesoup.readthedocs.io/en/v1.4/auto_examples/readers/Custom_Pandas_Dataloader.html#sphx-glr-auto-examples-readers-custom-pandas-dataloader-py)\n",
    "\n",
    "Continuing experiments - following tutorial #5: Probabilistic Data Association\n",
    "\n",
    "For this tutorial, I need to extend the test data set that I've been working with to include some clutter. I'll grab all plots within the timeframe of the test target that were not correlated to the test plane or any other targets of opportunity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from importlib import reload  # Python 3.4+\n",
    "from typing import Tuple\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "from math import ceil\n",
    "\n",
    "import dateutil\n",
    "from pymap3d import geodetic2enu\n",
    "\n",
    "import sys\n",
    "sys.path.append('C:/Users/ttrinter/git_repo/cspeed/data_common')\n",
    "sys.path.append('../../..')\n",
    "import data_functions as dfunc\n",
    "import visualizations as v\n",
    "from ttt_ss_funcs import generate_timestamps, ADSBTruthReader, CSVReaderXY, CSVReaderPolar, plot_all, CSVClutterReaderXY, group_plots\n",
    "\n",
    "\n",
    "from stonesoup.reader import DetectionReader, GroundTruthReader\n",
    "from stonesoup.base import Property\n",
    "from stonesoup.types.detection import Detection, Clutter\n",
    "from stonesoup.plotter import AnimatedPlotterly, Plotter\n",
    "\n",
    "from stonesoup.base import Property\n",
    "from stonesoup.buffered_generator import BufferedGenerator\n",
    "from stonesoup.functions import cart2sphere, sphere2cart\n",
    "from stonesoup.models.measurement.linear import LinearGaussian\n",
    "from stonesoup.models.measurement.nonlinear import CartesianToBearingRange\n",
    "from stonesoup.types.angle import Bearing\n",
    "from stonesoup.types.detection import Detection\n",
    "from stonesoup.types.groundtruth import GroundTruthState, GroundTruthPath\n",
    "\n",
    "# Tracker Imports\n",
    "from stonesoup.types.state import GaussianState\n",
    "\n",
    "plot_type = 'static' # or 'animated'\n",
    "\n",
    "sensor_positions = { 'RDU103': (51.52126391, 5.85862734)}\n",
    "\n",
    "METERS_in_NM = 1852\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/Read  to/from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "data_dir = 'C:/Users/ttrinter/git_repo/Stone-Soup/data'\n",
    "adsb_file = f'{data_dir}/adsb_straight.csv'\n",
    "adsb_data = pd.read_csv(adsb_file)\n",
    "adsb_data['timestamp'] = pd.to_datetime(adsb_data['timestamp'], errors='coerce')\n",
    "adsb_data = adsb_data.loc[~adsb_data['timestamp'].isna()]\n",
    "adsb_data['timestamp'] = pd.to_datetime(adsb_data['timestamp'], errors='coerce')\n",
    "adsb_data['timestamp'] = adsb_data['timestamp'].dt.tz_localize(None)\n",
    "\n",
    "rdp_file = f'{data_dir}/rdp_straight.csv'\n",
    "rdp_data = pd.read_csv(rdp_file)\n",
    "rdp_data['timestamp'] = pd.to_datetime(rdp_data['timestamp'], errors='coerce')\n",
    "rdp_data['timestamp'] = rdp_data['timestamp'].dt.tz_localize(None)\n",
    "\n",
    "# Matched Plots\n",
    "matched_csv = f'{data_dir}/rdp_matched2.csv'\n",
    "rdp_matched = pd.read_csv(matched_csv)\n",
    "rdp_matched['timestamp'] = pd.to_datetime(rdp_matched['timestamp'], errors='coerce')\n",
    "rdp_matched['timestamp'] = rdp_matched['timestamp'].dt.tz_localize(None)\n",
    "\n",
    "start_time = rdp_matched['timestamp'].min()\n",
    "end_time = rdp_matched['timestamp'].max()\n",
    "\n",
    "\n",
    "clutter_filename = f'{data_dir}/sample_clutter.csv'\n",
    "clutter_data = pd.read_csv(clutter_filename)\n",
    "\n",
    "print(f'ADSB: {len(adsb_data)}')\n",
    "print(f'RDP: {len(rdp_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.scatter_targets(clutter_data)\n",
    "plt.suptitle('Clutter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matched Data Set\n",
    "To make things even simpler, I'll grab the set of matched data for this test plane. Then most of the plots should be \"true\" detections. Let's see how the tracker does with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_address = 10537421\n",
    "file_dir = 'C:/Users/ttrinter/OneDrive - cspeed.com (1)/Documents/Data/Travis/2024-07-17'\n",
    "matched_file = '20240717_Travis_matched_rdp_61.xlsx'\n",
    "all_matched_data = pd.read_excel(f'{file_dir}/{matched_file}')\n",
    "matched_data = all_matched_data.loc[(all_matched_data.target_address==target_address) &\n",
    "                                (all_matched_data.close_enough==True)]\n",
    "matched_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_plot = v.plot_target_match2(matching=matched_data, \n",
    "                                    target_address=target_address, \n",
    "                                    plot_show=True, \n",
    "                                    pd_loc='title')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdp_matched['x_m'] = rdp_matched.x * METERS_in_NM\n",
    "rdp_matched['y_m'] = rdp_matched.y * METERS_in_NM\n",
    "rdp_matched.sort_values('timestamp', inplace=True)\n",
    "\n",
    "track_start_x = rdp_matched.iloc[0]['x_m']\n",
    "track_start_y = rdp_matched.iloc[0]['y_m']\n",
    "\n",
    "rdp_matched[['timestamp', 'rho','theta','x_m','y_m']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adsb = ADSBTruthReader.multiple_ground_truth_reader([adsb_file])\n",
    "adsb = ADSBTruthReader.single_ground_truth_reader(adsb_file)\n",
    "\n",
    "# Detections\n",
    "matched_xy = CSVReaderXY(matched_csv)\n",
    "matched_polar = CSVReaderPolar(matched_csv)\n",
    "\n",
    "# Cutter\n",
    "clutter = CSVClutterReaderXY(clutter_filename)\n",
    "\n",
    "dets = [next(iter(detection[1])) for detection in matched_xy.detections_gen()]\n",
    "cluts = [next(iter(detection[1])) for detection in clutter.detections_gen()]\n",
    "\n",
    "# Combine detections with clutter\n",
    "all_measurements = dets + cluts\n",
    "all_measurements.sort(key=lambda obj: obj.timestamp)\n",
    "\n",
    "plot_type='static'\n",
    "# plot_type='animated'\n",
    "timestamps = generate_timestamps(start_time, end_time)\n",
    "\n",
    "plot_all(start_time, \n",
    "         end_time,\n",
    "         all_measurements=all_measurements, \n",
    "        #  adsb=adsb,\n",
    "         plot_type='static')\n",
    "        # plot_type='animated')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Tutorial #7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stonesoup.models.transition.linear import CombinedLinearGaussianTransitionModel, \\\n",
    "                                               ConstantVelocity\n",
    "from stonesoup.predictor.kalman import KalmanPredictor\n",
    "from stonesoup.updater.kalman import KalmanUpdater\n",
    "from stonesoup.predictor.kalman import UnscentedKalmanPredictor\n",
    "from stonesoup.updater.kalman import UnscentedKalmanUpdater\n",
    "\n",
    "from stonesoup.types.track import Track\n",
    "from stonesoup.hypothesiser.distance import DistanceHypothesiser\n",
    "from stonesoup.measures import Mahalanobis, Euclidean\n",
    "\n",
    "from stonesoup.dataassociator.neighbour import NearestNeighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_variance = 50 # estimate of variance in m2 of state matrix elements (position and velocity)\n",
    "\n",
    "measurement_model = LinearGaussian(\n",
    "    ndim_state=4,   # Number of state dimensions (position and velocity in 2D)\n",
    "    mapping=(0, 2), # Mapping measurement vector index to state index\n",
    "    noise_covar=np.array([[default_variance, 0 ],  \n",
    "                          [0, default_variance]]),\n",
    "    seed=24\n",
    "    )  #Covariance matrix for Gaussian PDF\n",
    "\n",
    "# Transition Model\n",
    "q_const = 60\n",
    "q_x = q_const\n",
    "q_y = q_const\n",
    "transition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(q_x),\n",
    "                                                          ConstantVelocity(q_y)],\n",
    "                                                          seed=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stonesoup.hypothesiser.probability import PDAHypothesiser\n",
    "from stonesoup.dataassociator.probability import PDA\n",
    "from stonesoup.types.track import Track\n",
    "from stonesoup.types.array import StateVectors  # For storing state vectors during association\n",
    "from stonesoup.functions import gm_reduce_single  # For merging states to get posterior estimate\n",
    "from stonesoup.types.update import GaussianStateUpdate  # To store posterior estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prior\n",
    "prob_detect  = 0.85\n",
    "\n",
    "# predictor = KalmanPredictor(transition_model)\n",
    "# updater = KalmanUpdater(measurement_model)\n",
    "\n",
    "predictor = UnscentedKalmanPredictor(transition_model)\n",
    "updater = UnscentedKalmanUpdater(measurement_model)  # Keep alpha as default = 0.5\n",
    "\n",
    "hypothesiser = PDAHypothesiser(predictor=predictor,\n",
    "                               updater=updater,\n",
    "                               clutter_spatial_density=0.125,\n",
    "                               prob_detect=prob_detect)\n",
    "\n",
    "data_associator = PDA(hypothesiser=hypothesiser)\n",
    "\n",
    "# Clear things out from prior runs\n",
    "hypothesis = None\n",
    "post = None\n",
    "if \"track\" in globals():\n",
    "    del(track)\n",
    "\n",
    "# create a prior using the approximate start of the track\n",
    "prior = GaussianState([[track_start_x], [1], [track_start_y], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
    "\n",
    "\n",
    "grouped_sec, grouped_pass = group_plots(all_measurements)\n",
    "track = Track([prior])\n",
    "for n, measurements in enumerate(grouped_pass):\n",
    "    hypotheses = data_associator.associate({track},\n",
    "                                           measurements,\n",
    "                                           start_time + timedelta(seconds=n))\n",
    "\n",
    "    hypotheses = hypotheses[track]\n",
    "\n",
    "    # Loop through each hypothesis, creating posterior states for each, and merge to calculate\n",
    "    # approximation to actual posterior state mean and covariance.\n",
    "    posterior_states = []\n",
    "    posterior_state_weights = []\n",
    "    for hypothesis in hypotheses:\n",
    "        if not hypothesis:\n",
    "            posterior_states.append(hypothesis.prediction)\n",
    "        else:\n",
    "            posterior_state = updater.update(hypothesis)\n",
    "            posterior_states.append(posterior_state)\n",
    "        posterior_state_weights.append(\n",
    "            hypothesis.probability)\n",
    "\n",
    "    means = StateVectors([state.state_vector for state in posterior_states])\n",
    "    covars = np.stack([state.covar for state in posterior_states], axis=2)\n",
    "    weights = np.asarray(posterior_state_weights)\n",
    "\n",
    "    # Reduce mixture of states to one posterior estimate Gaussian.\n",
    "    post_mean, post_covar = gm_reduce_single(means, covars, weights)\n",
    "\n",
    "    # Add a Gaussian state approximation to the track.\n",
    "    track.append(GaussianStateUpdate(\n",
    "        post_mean, post_covar,\n",
    "        hypotheses,\n",
    "        hypotheses[0].measurement.timestamp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the ground truth and measurements with clutter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = rdp_matched['timestamp'].min()\n",
    "end_time = rdp_matched['timestamp'].max()\n",
    "meas_cart = CSVClutterReaderXY(matched_csv)\n",
    "adsb = ADSBTruthReader.multiple_ground_truth_reader([adsb_file])\n",
    "\n",
    "plot_all(start_time, end_time,\n",
    "         all_measurements=all_measurements, \n",
    "         adsb=adsb,  \n",
    "         tracks=[track], \n",
    "         plot_type='static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polar Coordinates\n",
    "Trying again, but changing the process to read rho and theta and, maybe later also radial velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Measurements - from Polar\n",
    "Recreate the all_measurements collection using the polar version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dets = [next(iter(detection[1])) for detection in matched_polar.detections_gen()]\n",
    "cluts = [next(iter(detection[1])) for detection in clutter.detections_gen()]\n",
    "\n",
    "# Combine detections with clutter\n",
    "all_measurements = dets + cluts\n",
    "all_measurements.sort(key=lambda obj: obj.timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kalman Filtering Again...\n",
    "Should be the same from here forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition Model\n",
    "q_const = 60\n",
    "q_x = q_const\n",
    "q_y = q_const\n",
    "transition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(q_x),\n",
    "                                                          ConstantVelocity(q_y)])\n",
    "\n",
    "# Create prior\n",
    "# predictor = KalmanPredictor(transition_model)\n",
    "# updater = KalmanUpdater(measurement_model)\n",
    "\n",
    "predictor = UnscentedKalmanPredictor(transition_model)\n",
    "updater = UnscentedKalmanUpdater(measurement_model)  # Keep alpha as default = 0.5\n",
    "\n",
    "hypothesiser = DistanceHypothesiser(predictor, updater, measure=Mahalanobis(), missed_distance=3)\n",
    "\n",
    "# Clear things out from prior runs\n",
    "hypothesis = None\n",
    "post = None\n",
    "if \"track\" in globals():\n",
    "    del(track)\n",
    "\n",
    "data_associator = NearestNeighbour(hypothesiser)\n",
    "\n",
    "# create a prior using the approximate start of the track\n",
    "prior = GaussianState([[track_start_x], [1], [track_start_y], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
    "\n",
    "# create a prior using the location of the radar\n",
    "# prior = GaussianState([[0], [q_const], [0], [q_const]], np.diag([default_variance, 0.5, default_variance, 0.5]), timestamp=start_time)\n",
    "\n",
    "# Loop through the predict, hypothesise, associate and update steps.\n",
    "\n",
    "grouped_sec, grouped_pass = group_plots(all_measurements)\n",
    "track = Track([prior])\n",
    "for n, measurements in enumerate(grouped_pass):\n",
    "    this_time = min(measurements, key=lambda meas: meas.timestamp).timestamp\n",
    "    this_time = this_time.replace(microsecond=0)\n",
    "    # print(f'{this_time}: {len(measurements)}')\n",
    "\n",
    "    if len(measurements)>0:\n",
    "        # print(f'{n}: {len(measurements)}')\n",
    "    # for n, measurements in enumerate(dets):\n",
    "        try: \n",
    "            hypotheses = data_associator.associate([track],\n",
    "                                                   measurements,\n",
    "                                                   this_time)\n",
    "            hypothesis = hypotheses[track]\n",
    "        \n",
    "            if hypothesis.measurement:\n",
    "                post = updater.update(hypothesis)\n",
    "                track.append(post)\n",
    "            else:  # When data associator says no detections are good enough, we'll keep the prediction\n",
    "                track.append(hypothesis.prediction)\n",
    "\n",
    "            # print(f'{this_time}: {len(measurements)}: SUCCESS')\n",
    "\n",
    "        except:\n",
    "            # print(f'{this_time}: {len(measurements)}: ERROR')\n",
    "            continue\n",
    "\n",
    "len(track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(start_time, \n",
    "         end_time,\n",
    "         all_measurements, \n",
    "         adsb,  \n",
    "         tracks=track, \n",
    "         plot_type='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very sensitive to the constant velocity parameter. They are also changing when re-running without making changes! I suspect the Kalman Filter is creating covariance matrices or something that are not getting reset when re-run.\n",
    "\n",
    "* 25: falls short of the ground truth consistently in the y- coordinate.\n",
    "* 35: does a pretty good job of matching the truth.\n",
    "* 50: matches well up until a point, then veers off track northerly for no apparent reason at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asterix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
