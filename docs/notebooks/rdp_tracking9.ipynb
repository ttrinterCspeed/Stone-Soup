{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDP #9 Initiators and Deleters\n",
    "[documentation](https://stonesoup.readthedocs.io/en/v0.1b6/auto_tutorials/09_Initiators_%26_Deleters.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from importlib import reload  # Python 3.4+\n",
    "from typing import Tuple\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "from math import ceil\n",
    "from ordered_set import OrderedSet\n",
    "\n",
    "import dateutil\n",
    "from pymap3d import geodetic2enu\n",
    "\n",
    "import sys\n",
    "sys.path.append('C:/Users/ttrinter/git_repo/cspeed/data_common')\n",
    "sys.path.append('../../..')\n",
    "import data_functions as dfunc\n",
    "import visualizations as v\n",
    "from ttt_ss_funcs import generate_timestamps, ADSBTruthReader, CSVReaderXY, CSVReaderPolar, plot_all, CSVClutterReaderXY, group_plots\n",
    "from ttt_ss_funcs import RDPandClutterInitiator\n",
    "\n",
    "from stonesoup.reader import DetectionReader, GroundTruthReader\n",
    "from stonesoup.base import Property\n",
    "from stonesoup.types.detection import Detection, Clutter\n",
    "from stonesoup.plotter import AnimatedPlotterly, Plotter\n",
    "\n",
    "from stonesoup.base import Property\n",
    "from stonesoup.buffered_generator import BufferedGenerator\n",
    "from stonesoup.functions import cart2sphere, sphere2cart\n",
    "from stonesoup.models.measurement.linear import LinearGaussian\n",
    "from stonesoup.models.measurement.nonlinear import CartesianToBearingRange\n",
    "from stonesoup.types.angle import Bearing\n",
    "from stonesoup.types.detection import Detection\n",
    "from stonesoup.types.groundtruth import GroundTruthState, GroundTruthPath\n",
    "\n",
    "# Tracker Imports\n",
    "from stonesoup.types.state import GaussianState\n",
    "\n",
    "plot_type = 'static' # or 'animated'\n",
    "\n",
    "sensor_positions = { 'RDU103': (51.52126391, 5.85862734)}\n",
    "\n",
    "METERS_in_NM = 1852\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_address1 = 10537421 # plane #1\n",
    "target_address2 = 10610889 # plane #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass Number\n",
    "All of the examples are looking at data once/second. That matches up the frequency of new plots. However, for our purposes, it may be better to look per radar pass, assuming that the radar will not see a target more than once per pass.\n",
    "\n",
    "I should be able to set the pass number on the plots and clutter using the cat 34 sector messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/Read  to/from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "data_dir = 'C:/Users/ttrinter/git_repo/Stone-Soup/data'\n",
    "\n",
    "# Now there are 2 ADSB Files\n",
    "adsb_data = pd.DataFrame()\n",
    "adsb_files = [f'{data_dir}/adsb_straight.csv', f'{data_dir}/adsb_straight2.csv']\n",
    "for file in adsb_files:\n",
    "    this_data = pd.read_csv(file)\n",
    "    this_data['timestamp'] = pd.to_datetime(this_data['timestamp'], errors='coerce')\n",
    "    this_data = this_data.loc[~this_data['timestamp'].isna()]\n",
    "    this_data['timestamp'] = pd.to_datetime(this_data['timestamp'], errors='coerce')\n",
    "    this_data['timestamp'] = this_data['timestamp'].dt.tz_localize(None)\n",
    "\n",
    "    adsb_data = pd.concat([adsb_data, this_data])\n",
    "\n",
    "# ADSB- first 3 min\n",
    "# adsb_file = f'{data_dir}/adsb3.csv'\n",
    "# adsb_data = pd.read_csv(adsb_file)\n",
    "\n",
    "rdp_file = f'{data_dir}/rdp_straight.csv'\n",
    "# rdp_straight['timestamp'] = pd.to_datetime(rdp_straight['timestamp'], errors='coerce')\n",
    "# rdp_straight['theta_rad'] = np.deg2rad(rdp_straight.theta)\n",
    "# rdp_straight.loc[rdp_straight.theta_rad>2*pi, 'theta_rad'] = rdp_straight.loc[rdp_straight.theta_rad>2*pi, 'theta_rad'] - 2*pi \n",
    "\n",
    "# rdp_straight = rdp_straight.loc[~rdp_straight['timestamp'].isna()]\n",
    "# rdp_straight.to_csv(rdp_file, index=False)\n",
    "# rdp_data = pd.read_csv(rdp_file)\n",
    "# rdp_data['timestamp'] = pd.to_datetime(rdp_data['timestamp'], errors='coerce')\n",
    "# rdp_data['timestamp'] = rdp_data['timestamp'].dt.tz_localize(None)\n",
    "# rdp_data['timestamp'] = rdp_data['timestamp'].astype('datetime64[us]')\n",
    "# rdp_data['timestamp'] = rdp_data['timestamp'].round('ms')\n",
    "\n",
    "\n",
    "# Matched Plots\n",
    "matched_csv1 = f'{data_dir}/rdp_matched.csv'\n",
    "matched_csv2 = f'{data_dir}/rdp_matched2.csv'\n",
    "matched_csv3min = f'{data_dir}/rdp_matched3.csv'\n",
    "matched_rdp_file = matched_csv2\n",
    "\n",
    "rdp_matched = pd.read_csv(matched_rdp_file)\n",
    "rdp_matched['timestamp'] = pd.to_datetime(rdp_matched['timestamp'], errors='coerce')\n",
    "rdp_matched['timestamp'] = rdp_matched['timestamp'].dt.tz_localize(None)\n",
    "rdp_matched['timestamp'] = rdp_matched['timestamp'].astype('datetime64[us]')\n",
    "rdp_matched['timestamp'] = rdp_matched['timestamp'].round('ms')\n",
    "\n",
    "start_time = rdp_matched['timestamp'].min()\n",
    "end_time = rdp_matched['timestamp'].max()\n",
    "\n",
    "clutter_file = f'{data_dir}/sample_clutter.csv'\n",
    "# clutter_file = f'{data_dir}/clutter3.csv'\n",
    "clutter_data = pd.read_csv(clutter_file)\n",
    "clutter_data['timestamp'] = pd.to_datetime(clutter_data['timestamp'], errors='coerce')\n",
    "clutter_data = clutter_data.loc[~clutter_data['timestamp'].isna()]\n",
    "clutter_data['timestamp'] = pd.to_datetime(clutter_data['timestamp'], errors='coerce')\n",
    "clutter_data['timestamp'] = clutter_data['timestamp'].dt.tz_localize(None)\n",
    "\n",
    "print(f'ADSB: {len(adsb_data)}')\n",
    "# print(f'RDP: {len(rdp_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{clutter_data.timestamp.min()} : {clutter_data.timestamp.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.scatter_targets(clutter_data)\n",
    "plt.suptitle('Clutter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matched Data Set\n",
    "To make things even simpler, I'll grab the set of matched data for this test plane. Then most of the plots should be \"true\" detections. Let's see how the tracker does with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_address1 = 10537421 # plane #1\n",
    "target_address2 = 10610889 # plane #\n",
    "target_addresses = [target_address1, target_address2]\n",
    "file_dir = 'C:/Users/ttrinter/OneDrive - cspeed.com (1)/Documents/Data/Travis/2024-07-17'\n",
    "matched_file = '20240717_Travis_matched_rdp_61.xlsx'\n",
    "all_matched_data = pd.read_excel(f'{file_dir}/{matched_file}')\n",
    "matched_data = all_matched_data.loc[(all_matched_data.target_address.isin(target_addresses)) &\n",
    "                                (all_matched_data.close_enough==True) & \n",
    "                                (all_matched_data.timestamp_adsb>=start_time) & \n",
    "                                (all_matched_data.timestamp_adsb<=end_time)]\n",
    "matched_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_plot = v.plot_target_match2(matching=matched_data, \n",
    "                                    target_address=target_address1, \n",
    "                                    plot_show=True, \n",
    "                                    pd_loc='title')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_plot = v.plot_target_match2(matching=matched_data, \n",
    "                                    target_address=target_address2, \n",
    "                                    plot_show=True, \n",
    "                                    pd_loc='title')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Group by the 'category' column and plot each group separately\n",
    "for tgt, group in rdp_matched.groupby('target_address'):\n",
    "    ax.scatter(group['x'], group['y'], label=tgt, marker=\"+\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Matched RDP Plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adsb = ADSBTruthReader.multiple_ground_truth_reader(adsb_files)\n",
    "# adsb = ADSBTruthReader.single_ground_truth_reader(adsb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = Plotter()\n",
    "plotter.plot_ground_truths(adsb, \n",
    "                        mapping=[0, 2], \n",
    "                        markersize = 5,\n",
    "                        marker = 's', \n",
    "                        markerfacecolor = 'none', \n",
    "                        alpha = 0.2)\n",
    "plt.grid()\n",
    "plt.title(\"Two Targets of Opportunity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detections\n",
    "matched_xy = CSVReaderXY(matched_rdp_file)\n",
    "matched_polar = CSVReaderPolar(matched_rdp_file)\n",
    "\n",
    "# Cutter\n",
    "clutter = CSVClutterReaderXY(clutter_file)\n",
    "\n",
    "dets = [next(iter(detection[1])) for detection in matched_xy.detections_gen()]\n",
    "cluts = [next(iter(detection[1])) for detection in clutter.detections_gen()]\n",
    "\n",
    "# Combine detections with clutter\n",
    "all_measurements = dets + cluts\n",
    "all_measurements.sort(key=lambda obj: obj.timestamp)\n",
    "\n",
    "plot_all(start_time, \n",
    "         end_time,\n",
    "         all_measurements=all_measurements, \n",
    "         adsb=adsb, \n",
    "         plot_type='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Tutorial #9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stonesoup.models.transition.linear import CombinedLinearGaussianTransitionModel, \\\n",
    "                                               ConstantVelocity\n",
    "\n",
    "from stonesoup.measures import Mahalanobis\n",
    "from stonesoup.predictor.kalman import KalmanPredictor\n",
    "from stonesoup.updater.kalman import KalmanUpdater\n",
    "\n",
    "from stonesoup.predictor.kalman import ExtendedKalmanPredictor\n",
    "from stonesoup.updater.kalman import ExtendedKalmanUpdater\n",
    "\n",
    "from stonesoup.predictor.kalman import UnscentedKalmanPredictor\n",
    "from stonesoup.updater.kalman import UnscentedKalmanUpdater\n",
    "\n",
    "from stonesoup.types.track import Track\n",
    "from stonesoup.hypothesiser.distance import DistanceHypothesiser\n",
    "\n",
    "from stonesoup.dataassociator.neighbour import NearestNeighbour\n",
    "from stonesoup.dataassociator.neighbour import GNNWith2DAssignment\n",
    "from stonesoup.deleter.error import CovarianceBasedDeleter\n",
    "\n",
    "from stonesoup.types.state import GaussianState\n",
    "from stonesoup.initiator.simple import MultiMeasurementInitiator\n",
    "from stonesoup.deleter.time import UpdateTimeDeleter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measurement Model\n",
    "np.random.seed(42)\n",
    "default_variance = 50\n",
    "\n",
    "measurement_model = LinearGaussian(\n",
    "    ndim_state=4,   # Number of state dimensions (position and velocity in 2D)\n",
    "    mapping=(0, 2), # Mapping measurement vector index to state index\n",
    "    noise_covar=np.array([[default_variance, 0 ],  \n",
    "                          [0, default_variance]])\n",
    "    )  #Covariance matrix for Gaussian PDF\n",
    "\n",
    "# Transition Model\n",
    "q_const = 60\n",
    "q_x = q_const\n",
    "q_y = q_const\n",
    "transition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(q_x),\n",
    "                                                          ConstantVelocity(q_y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = ExtendedKalmanPredictor(transition_model)\n",
    "updater = ExtendedKalmanUpdater(measurement_model)\n",
    "\n",
    "#Hypothesisers\n",
    "hypothesiser = DistanceHypothesiser(predictor, updater, measure=Mahalanobis(), missed_distance=3)\n",
    "# init_hypothesiser = DistanceHypothesiser(predictor, updater, measure=Mahalanobis(), missed_distance=2)\n",
    "\n",
    "#Data Associators\n",
    "data_associator = NearestNeighbour(hypothesiser)\n",
    "# data_associator = GNNWith2DAssignment(hypothesiser)\n",
    "# init_data_associator = GNNWith2DAssignment(init_hypothesiser)\n",
    "\n",
    "# Deleter\n",
    "# deleter =  CovarianceBasedDeleter(covar_trace_thresh=4)\n",
    "deleter = UpdateTimeDeleter(time_since_update=timedelta(seconds=30), delete_last_pred=True)\n",
    "# init_deleter = UpdateTimeDeleter(time_since_update=timedelta(seconds=15), delete_last_pred=True)\n",
    "\n",
    "prior_state = GaussianState([0, q_x, 0, q_y], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
    "# prior_state = GaussianState([0, 1, 0, 1], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
    "initiator = MultiMeasurementInitiator(\n",
    "    prior_state=prior_state, \n",
    "    measurement_model=measurement_model, \n",
    "    deleter=deleter, \n",
    "    data_associator=data_associator, \n",
    "    updater=updater, \n",
    "    min_points=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = set()\n",
    "\n",
    "grouped_sec, grouped_pass = group_plots(all_measurements)\n",
    "\n",
    "# for n, detections in enumerate(grouped_sec):\n",
    "for n, detections in enumerate(grouped_pass):    \n",
    "    this_time = min(detections, key=lambda detct: detct.timestamp).timestamp\n",
    "    this_time = this_time.replace(microsecond=0)\n",
    "\n",
    "    # Calculate all hypothesis pairs and associate the elements in the best subset to the tracks.\n",
    "    if len(detections)>0:\n",
    "        hypotheses = data_associator.associate(tracks, detections, this_time)\n",
    "        \n",
    "        associated_detections = set()\n",
    "        for track in tracks:\n",
    "            hypothesis = hypotheses[track]\n",
    "            if hypothesis.measurement:\n",
    "                post = updater.update(hypothesis)\n",
    "                track.append(post)\n",
    "                associated_detections.add(hypothesis.measurement)\n",
    "            else:\n",
    "                track.append(hypothesis.prediction)\n",
    "\n",
    "        tracks -= deleter.delete_tracks(tracks = tracks)\n",
    "        tracks |= initiator.initiate(detections - associated_detections, this_time)\n",
    "        \n",
    "        print(f'{n}: {len(tracks)} tracks')\n",
    "\n",
    "    \n",
    "print(f'{len(tracks)} tracks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the ground truth and measurements with clutter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(start_time,\n",
    "         end_time,\n",
    "         all_measurements=all_measurements, \n",
    "         adsb=adsb,\n",
    "         tracks=tracks,\n",
    "        #  plot_type='static')\n",
    "        plot_type='animated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Track Initators and Delters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Target-Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = MultiTargetTracker(\n",
    "    detector=detector,\n",
    "    initiator=initiator,\n",
    "    deleter=deleter,\n",
    "    data_associator=data_associator,\n",
    "    updater=updater,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = set()\n",
    "detections = set()\n",
    "# try:\n",
    "for time, ctracks in tracker:\n",
    "    # print(f'{time}:{len(tracks)}')\n",
    "    tracks |= ctracks\n",
    "    detections |= tracker.detector.detections\n",
    "# except:\n",
    "#     pass\n",
    "len(tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tracker did a good job of following each of the tracks separately! However, the more vertical track was wierdly combined with what appears to be an entirely different track in another location and direction.\n",
    "\n",
    "Similar to the previous analysis - after running the polar tracks, re-running these cartesian tracks no longer works! Clearly something is lingering after a run. Not all inputs are getting reset with a re-run. Need to sort that out!\n",
    "\n",
    "It is also suspect that 14 correlated plots get passed and are not added to the first track. It is not clear what/why these plots are skipped. Need to investigate that too. Perhaps with a smaller dataset that only inlcudes the first 20 correlated plots or something.\n",
    "\n",
    "Maybe the subsequent tutorials will address track deletions and initiation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polar Coordinates\n",
    "Trying again, but changing the process to read rho and theta and, maybe later also radial velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detections\n",
    "meas_polar = CSVReaderPolar(matched_csv2)\n",
    "\n",
    "dets = [next(iter(detection[1])) for detection in meas_polar.detections_gen()]\n",
    "cluts = [next(iter(detection[1])) for detection in clutter.detections_gen()]\n",
    "\n",
    "# Combine detections with clutter\n",
    "all_measurements = dets + cluts\n",
    "all_measurements.sort(key=lambda obj: obj.timestamp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kalman Filtering Again...\n",
    "Should be the same from here forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition Model\n",
    "q_const = 35\n",
    "q_x = q_const\n",
    "q_y = q_const\n",
    "default_variance=50\n",
    "\n",
    "transition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(q_x),\n",
    "                                                          ConstantVelocity(q_y)])\n",
    "\n",
    "# measurement_model = CartesianToBearingRange(ndim_state=4, \n",
    "#                                             mapping=(0,2), \n",
    "#                                             noise_covar=np.array([[default_variance, 0 ],\n",
    "#                                                                   [0, default_variance]]))\n",
    "\n",
    "# Basic Kalman\n",
    "predictor = KalmanPredictor(transition_model)\n",
    "updater = KalmanUpdater(measurement_model)\n",
    "\n",
    "#Extended Kalman\n",
    "predictor = ExtendedKalmanUpdater(transition_model)\n",
    "updater = ExtendedKalmanUpdater(measurement_model)\n",
    "\n",
    "#Unscented Kalman\n",
    "# predictor = UnscentedKalmanPredictor(transition_model)\n",
    "# updater = UnscentedKalmanUpdater(measurement_model)  # Keep alpha as default = 0.5\n",
    "\n",
    "hypothesiser = DistanceHypothesiser(predictor, updater, measure=Mahalanobis(), missed_distance=3)\n",
    "data_associator = NearestNeighbour(hypothesiser)\n",
    "\n",
    "# create a prior using the approximate start of the track\n",
    "# prior = GaussianState([[track_start_x], [1], [track_start_y], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
    "\n",
    "prior1 = GaussianState([[track_start1_x], [1], [track_start1_y], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
    "prior2 = GaussianState([[track_start2_x], [1], [track_start2_y], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
    "\n",
    "# create a prior using the location of the radar\n",
    "# prior = GaussianState([[0], [q_const], [0], [q_const]], np.diag([default_variance, 0.5, default_variance, 0.5]), timestamp=start_time)\n",
    "\n",
    "# Loop through the predict, hypothesise, associate and update steps.\n",
    "# del(tracks)\n",
    "if \"tracks\" in globals():\n",
    "    del tracks\n",
    "\n",
    "if \"track\" in globals():\n",
    "    del(track)\n",
    "\n",
    "tracks = {Track([prior1]), Track([prior2])}\n",
    "\n",
    "grouped_sec, grouped_pass = group_plots(all_measurements)\n",
    "\n",
    "# for n, measurements in enumerate(grouped_sec):\n",
    "for n, measurements in enumerate(grouped_pass):    \n",
    "    this_time = min(measurements, key=lambda meas: meas.timestamp).timestamp\n",
    "    this_time = this_time.replace(microsecond=0)\n",
    "\n",
    "    # Calculate all hypothesis pairs and associate the elements in the best subset to the tracks.\n",
    "    if len(measurements)>0:\n",
    "    # for n, measurements in enumerate(dets):\n",
    "        try: \n",
    "            hypotheses = data_associator.associate(tracks,\n",
    "                                                measurements,\n",
    "                                                this_time)\n",
    "            for track in tracks:\n",
    "                hypothesis = hypotheses[track]\n",
    "                if hypothesis.measurement:\n",
    "                    post = updater.update(hypothesis)\n",
    "                    track.append(post)\n",
    "                else:  # When data associator says no detections are good enough, we'll keep the prediction\n",
    "                    track.append(hypothesis.prediction)\n",
    "\n",
    "        except:\n",
    "            # print(f'{this_time}: {len(measurements)}: ERROR')\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(start_time,\n",
    "         end_time,\n",
    "         all_measurements=all_measurements, \n",
    "         adsb=adsb,\n",
    "         tracks=tracks,\n",
    "         plot_type='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is unclear why the cartesian tracker worked reasonably well (besides continuing the track with a large change in direction and location) and the polar tracker didn't ever continue the track.\n",
    "\n",
    "Leaving it for the next tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grouped_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cspeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
