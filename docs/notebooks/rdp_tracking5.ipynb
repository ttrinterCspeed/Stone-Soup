{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDP - Stone Soup #5: Data Association with Clutter \n",
    "[Documentation](https://stonesoup.readthedocs.io/en/v1.4/auto_examples/readers/Custom_Pandas_Dataloader.html#sphx-glr-auto-examples-readers-custom-pandas-dataloader-py)\n",
    "\n",
    "Continuing experiments - following tutorial #5: Data Association with clutter.\n",
    "\n",
    "For this tutorial, I need to extend the test data set that I've been working with to include some clutter. I'll grab all plots within the timeframe of the test target that were not correlated to the test plane or any other targets of opportunity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from importlib import reload  # Python 3.4+\n",
    "from typing import Tuple\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "from math import ceil\n",
    "\n",
    "import dateutil\n",
    "from pymap3d import geodetic2enu\n",
    "\n",
    "import sys\n",
    "sys.path.append('C:/Users/ttrinter/git_repo/cspeed/data_common')\n",
    "sys.path.append('../../..')\n",
    "import data_functions as dfunc\n",
    "import visualizations as v\n",
    "from ttt_ss_funcs import generate_timestamps, ADSBTruthReader, CSVReaderXY, CSVReaderPolar, plot_all, CSVClutterReaderXY\n",
    "\n",
    "\n",
    "from stonesoup.reader import DetectionReader, GroundTruthReader\n",
    "from stonesoup.base import Property\n",
    "from stonesoup.types.detection import Detection, Clutter\n",
    "from stonesoup.plotter import AnimatedPlotterly, Plotter\n",
    "\n",
    "from stonesoup.base import Property\n",
    "from stonesoup.buffered_generator import BufferedGenerator\n",
    "from stonesoup.functions import cart2sphere, sphere2cart\n",
    "from stonesoup.models.measurement.linear import LinearGaussian\n",
    "from stonesoup.models.measurement.nonlinear import CartesianToBearingRange\n",
    "from stonesoup.types.angle import Bearing\n",
    "from stonesoup.types.detection import Detection\n",
    "from stonesoup.types.groundtruth import GroundTruthState, GroundTruthPath\n",
    "\n",
    "# Tracker Imports\n",
    "from stonesoup.types.state import GaussianState\n",
    "\n",
    "plot_type = 'static' # or 'animated'\n",
    "\n",
    "sensor_positions = { 'RDU103': (51.52126391, 5.85862734)}\n",
    "\n",
    "METERS_in_NM = 1852\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data from BigQuery\n",
    "* rdp_straight: short, straight flight path\n",
    "* rdp_extended: longer flight path\n",
    "* adsb_straight: truth for rdp_straight\n",
    "* adsb_extended: truth for rdp_extended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_address = 10537421\n",
    "# adsb_sql = f\"\"\"SELECT `timestamp`,\n",
    "#         time_of_day, \n",
    "#         latitude, \n",
    "#         longitude, \n",
    "#         target_address,\n",
    "#         flight_level, \n",
    "#         rho, \n",
    "#         theta\n",
    "# FROM radar_data.adsb\n",
    "# WHERE test_date = '2024-07-17'\n",
    "# AND target_address={target_address}\n",
    "# and latitude is not NULL\n",
    "# AND rho<20\n",
    "# ORDER BY `timestamp`\"\"\"\n",
    "\n",
    "# adsb_straight = dfunc.query_to_df(adsb_sql)\n",
    "\n",
    "# rdp_sql = f\"\"\"SELECT \n",
    "#         `timestamp`,\n",
    "#         time_of_day,\n",
    "#         cal, \n",
    "#         rho,\n",
    "#         theta, \n",
    "#         x, \n",
    "#         y, \n",
    "#         field_note \n",
    "# FROM radar_data.rdp\n",
    "\n",
    "# --Clutter Where Statement\n",
    "# WHERE test_date = '2024-07-17'\n",
    "# AND `timestamp` >= '{start_time.strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "# AND `timestamp` <= '{end_time.strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "# AND sortie_id=61\n",
    "# ORDER BY `timestamp`\"\"\"\n",
    "\n",
    "# # # Test Plane Where\n",
    "# # # WHERE `timestamp` >= '{adsb_straight.timestamp.min().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "# # # AND `timestamp` <= '{adsb_straight.timestamp.max().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "# # # AND rho >= {adsb_straight.rho.min()-0.5}\n",
    "# # # AND rho <= {adsb_straight.rho.max()+0.5}\n",
    "# # # AND theta >= {adsb_straight.theta.min()- 5}\n",
    "# # # AND theta <= {adsb_straight.theta.max()+5}\"\"\"\n",
    "\n",
    "# # # rdp_straight = dfunc.query_to_df(rdp_sql)\n",
    "# # # rdp_straight.head()\n",
    "\n",
    "# rdp_clutter = dfunc.query_to_df(rdp_sql)\n",
    "# rdp_clutter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather Clutter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove timezone from timestamps\n",
    "# rdp_clutter['timestamp'] = rdp_clutter['timestamp'].dt.tz_localize(None)\n",
    "# rdp_clutter['timestamp'] = rdp_clutter['timestamp'].astype('datetime64[us]')\n",
    "# rdp_clutter['timestamp'] = rdp_clutter['timestamp'].round('ms')\n",
    "\n",
    "# all_matched_data['timestamp'] = all_matched_data['timestamp_rdp'].dt.tz_localize(None)\n",
    "# all_matched_data['timestamp'] = all_matched_data['timestamp'].astype('datetime64[us]')\n",
    "# all_matched_data['timestamp'] = all_matched_data['timestamp'].round('ms')\n",
    "\n",
    "# # # Select only needed columns\n",
    "# cols_to_keep = ['timestamp','rho','theta','cal','x','y']\n",
    "# rdp_clutter = rdp_clutter[cols_to_keep]\n",
    "\n",
    "# # Left JOIN with the matched data to eliminate all RDP plots matched to any targets of opportunity\n",
    "# rdp_clutter = pd.merge(rdp_clutter, all_matched_data.loc[all_matched_data.close_enough==1], \n",
    "#                                left_on=['rho','theta','timestamp'], \n",
    "#                                right_on = ['rho_rdp','theta_rdp','timestamp'], \n",
    "#                                how='left')\n",
    "\n",
    "# # Drop rows that matched a target of opportunity\n",
    "# rdp_clutter = rdp_clutter.loc[rdp_clutter.rho_rdp.isna()]\n",
    "# # filter for only the range where the test plane is to simplify things\n",
    "# rdp_clutter = rdp_clutter.loc[(rdp_clutter.rho>=matched_data.rho_rdp.min()) &\n",
    "#                               (rdp_clutter.rho<=matched_data.rho_rdp.max())&\n",
    "#                               (rdp_clutter.theta>=matched_data.theta_rdp.min()) &\n",
    "#                               (rdp_clutter.theta<=matched_data.theta_rdp.max()) & \n",
    "#                               (rdp_clutter['timestamp'] >= matched_data.timestamp_rdp.min()) &\n",
    "#                               (rdp_clutter['timestamp'] <= matched_data.timestamp_rdp.max())]\n",
    "\n",
    "# rdp_clutter['theta_rad'] = np.deg2rad(rdp_clutter.theta)\n",
    "\n",
    "# data_dir = 'C:/Users/ttrinter/git_repo/Stone-Soup/data'\n",
    "# clutter_filename = f'{data_dir}/sample_clutter.csv'\n",
    "# rdp_clutter.to_csv(clutter_filename, index=False)\n",
    "# len(rdp_clutter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/Read  to/from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "data_dir = 'C:/Users/ttrinter/git_repo/Stone-Soup/data'\n",
    "adsb_file = f'{data_dir}/adsb_straight.csv'\n",
    "# adsb_straight.to_csv(adsb_file, index=False)\n",
    "adsb_data = pd.read_csv(adsb_file)\n",
    "adsb_data['timestamp'] = pd.to_datetime(adsb_data['timestamp'], errors='coerce')\n",
    "adsb_data = adsb_data.loc[~adsb_data['timestamp'].isna()]\n",
    "adsb_data['timestamp'] = pd.to_datetime(adsb_data['timestamp'], errors='coerce')\n",
    "adsb_data['timestamp'] = adsb_data['timestamp'].dt.tz_localize(None)\n",
    "\n",
    "rdp_file = f'{data_dir}/rdp_straight.csv'\n",
    "# rdp_straight['timestamp'] = pd.to_datetime(rdp_straight['timestamp'], errors='coerce')\n",
    "# rdp_straight['theta_rad'] = np.deg2rad(rdp_straight.theta)\n",
    "# rdp_straight.loc[rdp_straight.theta_rad>2*pi, 'theta_rad'] = rdp_straight.loc[rdp_straight.theta_rad>2*pi, 'theta_rad'] - 2*pi \n",
    "\n",
    "# rdp_straight = rdp_straight.loc[~rdp_straight['timestamp'].isna()]\n",
    "# rdp_straight.to_csv(rdp_file, index=False)\n",
    "rdp_data = pd.read_csv(rdp_file)\n",
    "rdp_data['timestamp'] = pd.to_datetime(rdp_data['timestamp'], errors='coerce')\n",
    "rdp_data['timestamp'] = rdp_data['timestamp'].dt.tz_localize(None)\n",
    "\n",
    "# Matched Plots\n",
    "matched_csv = f'{data_dir}/rdp_matched.csv'\n",
    "rdp_matched = pd.read_csv(matched_csv)\n",
    "rdp_matched['timestamp'] = pd.to_datetime(rdp_matched['timestamp'], errors='coerce')\n",
    "rdp_matched['timestamp'] = rdp_matched['timestamp'].dt.tz_localize(None)\n",
    "\n",
    "start_time = rdp_matched['timestamp'].min()\n",
    "end_time = rdp_matched['timestamp'].max()\n",
    "\n",
    "\n",
    "clutter_filename = f'{data_dir}/sample_clutter.csv'\n",
    "clutter_data = pd.read_csv(clutter_filename)\n",
    "\n",
    "print(f'ADSB: {len(adsb_data)}')\n",
    "print(f'RDP: {len(rdp_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.scatter_targets(clutter_data)\n",
    "plt.suptitle('Clutter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matched Data Set\n",
    "To make things even simpler, I'll grab the set of matched data for this test plane. Then most of the plots should be \"true\" detections. Let's see how the tracker does with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = 'C:/Users/ttrinter/OneDrive - cspeed.com (1)/Documents/Data/Travis/2024-07-17'\n",
    "matched_file = '20240717_Travis_matched_rdp_61.xlsx'\n",
    "all_matched_data = pd.read_excel(f'{file_dir}/{matched_file}')\n",
    "matched_data = all_matched_data.loc[(all_matched_data.target_address==target_address) &\n",
    "                                (all_matched_data.close_enough==True)]\n",
    "matched_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_plot = v.plot_target_match2(matching=matched_data, \n",
    "                                    target_address=target_address, \n",
    "                                    plot_show=True, \n",
    "                                    pd_loc='title')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdp_matched = matched_data[['timestamp_rdp',\n",
    "                            'cal_rdp',\n",
    "                            'rho_rdp',\n",
    "                            'theta_rdp', \n",
    "                            'target_address']]\n",
    "\n",
    "rdp_matched['theta_rad'] = np.deg2rad(rdp_matched.theta_rdp)\n",
    "rdp_matched.rename(columns={'rho_rdp': 'rho',\n",
    "                            'theta_rdp': 'theta', \n",
    "                            'timestamp_rdp': 'timestamp', \n",
    "                            'cal_rdp': 'cal'}, \n",
    "                            inplace=True)\n",
    "\n",
    "rdp_matched['x'], rdp_matched['y'] = zip(*rdp_matched.apply(lambda x: dfunc.polar_to_cartesian(x.rho, x.theta), axis=1))\n",
    "\n",
    "matched_csv = f'{data_dir}/rdp_matched.csv'\n",
    "rdp_matched.to_csv(matched_csv, index=False)\n",
    "\n",
    "rdp_matched.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdp_matched['x_m'] = rdp_matched.x * METERS_in_NM\n",
    "rdp_matched['y_m'] = rdp_matched.y * METERS_in_NM\n",
    "rdp_matched.sort_values('timestamp', inplace=True)\n",
    "\n",
    "track_start_x = rdp_matched.iloc[0]['x_m']\n",
    "track_start_y = rdp_matched.iloc[0]['y_m']\n",
    "\n",
    "rdp_matched[['timestamp', 'rho','theta','x_m','y_m']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adsb = ADSBTruthReader.multiple_ground_truth_reader([adsb_file])\n",
    "\n",
    "# Detections\n",
    "matched_xy = CSVReaderXY(matched_csv)\n",
    "matched_polar = CSVReaderPolar(matched_csv)\n",
    "\n",
    "# Cutter\n",
    "clutter = CSVClutterReaderXY(clutter_filename)\n",
    "\n",
    "dets = [next(iter(detection[1])) for detection in matched_xy.detections_gen()]\n",
    "cluts = [next(iter(detection[1])) for detection in clutter.detections_gen()]\n",
    "\n",
    "# Combine detections with clutter\n",
    "all_measurements = dets + cluts\n",
    "all_measurements.sort(key=lambda obj: obj.timestamp)\n",
    "\n",
    "plot_type='static'\n",
    "# plot_type='animated'\n",
    "timestamps = generate_timestamps(start_time, end_time)\n",
    "\n",
    "# plot_all(all_measurements, adsb, start_time, end_time, plot_type='static')\n",
    "plot_all(all_measurements, adsb, start_time, end_time, plot_type='animated')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Tutorial #5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of detection\n",
    "For the first time we introduce the possibility that, at any time-step, our sensor receives no detection from the target (i.e. $p_d < 1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_det = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate clutter\n",
    "Next in the tutorial, they simulate clutter. Using LWR data, we have plenty of clutter to work with. For this example, I've taken all of the RDP data within the same timeframe, range and azimuth ranges for the test plane and removed all plots associated with any targets of opportunity, including the test plane. Looking at the plot of remaining \"clutter,\" it is clear that some of those plots belong to a track. Those aircraft were either:\n",
    "1. not squawking its position and therefore missing from our 'truth' data\n",
    "2. not captured by the ADS-B recorder, again missing from the 'truth'\n",
    "2. not correctly correlated to the track \n",
    "3. correlated to the track, but outside of the designated range and azimuth tolerances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Hypothesiser and Nearest Neighbour\n",
    "Perhaps the simplest way to associate a detection with a prediction is to measure a 'distance' to each detection and hypothesise that the detection with the lowest distance\n",
    "is correctly associated with that prediction.\n",
    "\n",
    "An appropriate distance metric for states described by Gaussian distributions is the *Mahalanobis distance*. This quantifies the distance of a point relative to a given distribution. In the case of a point $\\mathbf{x} = [x_{1}, ..., x_{N}]^T$, and distribution with mean $\\boldsymbol{\\mu} = [\\mu_{1}, ..., \\mu_{N}]^T$ and covariance matrix $P$, the Mahalanobis distance of $\\mathbf{x}$ from the distribution is given by:\n",
    "\n",
    "$$\n",
    "       \\sqrt{(\\mathbf{x} - \\boldsymbol{\\mu})^T P^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})}\n",
    "$$\n",
    "which equates to the multi-dimensional measure of how many standard deviations a point is away from the mean.\n",
    "\n",
    "We're going to create a hypothesiser that ranks detections against predicted measurement according to the Mahalanobis distance, where those that fall outside of :math:`3` standard deviations of the predicted measurement's mean are ignored. To do this we create a `DistanceHypothesiser` which pairs incoming detections with track predictions, and pass it a `Measure` class which (in this instance) calculates the Mahalanobis distance.\n",
    "\n",
    "The hypothesiser must use a predicted state given by the predictor, create a measurement prediction using the updater, and compare this to a detection given a specific metric. Hence, it takes the predictor, updater, measure (metric) and missed distance (gate) as its arguments. We therefore need to create a predictor and updater, and to initialise a measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stonesoup.models.transition.linear import CombinedLinearGaussianTransitionModel, \\\n",
    "                                               ConstantVelocity\n",
    "from stonesoup.predictor.kalman import KalmanPredictor\n",
    "from stonesoup.updater.kalman import KalmanUpdater\n",
    "from stonesoup.predictor.kalman import UnscentedKalmanPredictor\n",
    "from stonesoup.updater.kalman import UnscentedKalmanUpdater\n",
    "\n",
    "from stonesoup.types.track import Track\n",
    "from stonesoup.hypothesiser.distance import DistanceHypothesiser\n",
    "from stonesoup.measures import Mahalanobis, Euclidean\n",
    "\n",
    "from stonesoup.dataassociator.neighbour import NearestNeighbour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the `NearestNeighbour` data associator, which picks the hypothesis pair (predicted measurement and detection) with the highest 'score' (in this instance, those that are closest to each other).\n",
    "\n",
    " <div>\n",
    " <img src=\"https://stonesoup.readthedocs.io/en/latest/_images/NN_Association_Diagram.png\" width=\"500\"/>\n",
    " <div>\n",
    "\n",
    " In the diagram above, there are three possible detections to be considered for association (some of which may be clutter). The detection with a score of `0.4` is selected by the nearest neighbour algorithm.\n",
    "\n",
    "## Run the Kalman filter with the associator\n",
    "With these components, we can run the simulated data and clutter through the Kalman filter. However, the process is expecting the data in sets by second where the true detections and clutter are sets grouped by second. We need to reshape our data to match that format for the data associater to work properly.\n",
    "\n",
    "***Note:*** The per-second nature of the sample might not be the best way to evaluate real data. Per-scan, or per-sector may make more sense. On a per-scan basis, we'd expect to see each active target in every scan. If looking by sector, we'd only expect to see each track updated every 32 sectors, or thereabouts, assuming the radar will not see a target twice in one time around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Group objects by the second of their timestamp\n",
    "grouped_objects = defaultdict(set)\n",
    "\n",
    "for meas in all_measurements:\n",
    "# for meas in dets:\n",
    "    # Get the second part of the timestamp\n",
    "    timestamp_s = meas.timestamp.replace(microsecond=0)  # Truncate to second precision\n",
    "    grouped_objects[timestamp_s].add(meas)\n",
    "    # print(timestamp_s)\n",
    "\n",
    "# Convert grouped objects to a set of sets\n",
    "grouped_measurements = set(frozenset(group) for group in sorted(grouped_objects.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_list = [group for group in grouped_objects.values()]\n",
    "# grouped_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measurement Model\n",
    "np.random.seed(42)\n",
    "\n",
    "default_variance = 50 # estimate of variance in m2 of state matrix elements (position and velocity)\n",
    "\n",
    "measurement_model = LinearGaussian(\n",
    "    ndim_state=4,   # Number of state dimensions (position and velocity in 2D)\n",
    "    mapping=(0, 2), # Mapping measurement vector index to state index\n",
    "    noise_covar=np.array([[default_variance, 0 ],  \n",
    "                          [0, default_variance]]),\n",
    "    seed=24\n",
    "    )  #Covariance matrix for Gaussian PDF\n",
    "\n",
    "# Transition Model\n",
    "q_const = 40\n",
    "q_x = q_const\n",
    "q_y = q_const\n",
    "transition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(q_x),\n",
    "                                                          ConstantVelocity(q_y)],\n",
    "                                                          seed=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure why, but running a second time works much better. Something must be getting calibrated the first time\n",
    "# and lingering to make the second track better. Need to identify that something and, if possible, set it as an\n",
    "# initial condition.\n",
    "for i in range (0,1): \n",
    "    # Create prior\n",
    "    predictor = KalmanPredictor(transition_model)\n",
    "    updater = KalmanUpdater(measurement_model)\n",
    "\n",
    "    # predictor = UnscentedKalmanPredictor(transition_model)\n",
    "    # updater = UnscentedKalmanUpdater(measurement_model)  # Keep alpha as default = 0.5\n",
    "\n",
    "    hypothesiser = DistanceHypothesiser(predictor, updater, measure=Mahalanobis(), missed_distance=3)\n",
    "    # hypothesiser = DistanceHypothesiser(predictor, updater, measure=Euclidean(), missed_distance=80)\n",
    "    data_associator = NearestNeighbour(hypothesiser)\n",
    "\n",
    "    # Clear things out from prior runs\n",
    "    hypothesis = None\n",
    "    post = None\n",
    "    if \"track\" in globals():\n",
    "        del(track)\n",
    "\n",
    "    # create a prior using the approximate start of the track\n",
    "    prior = GaussianState([[track_start_x], [1], [track_start_y], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
    "\n",
    "    # create a prior using the location of the radar\n",
    "    # prior = GaussianState([[0], [q_const], [0], [q_const]], np.diag([default_variance, 0.5, default_variance, 0.5]), timestamp=start_time)\n",
    "\n",
    "    # Loop through the predict, hypothesise, associate and update steps.\n",
    "\n",
    "    track = Track([prior])\n",
    "    for n, measurements in enumerate(grouped_list):\n",
    "        this_time = min(measurements, key=lambda meas: meas.timestamp).timestamp\n",
    "        this_time = this_time.replace(microsecond=0)\n",
    "        # print(f'{this_time}: {len(measurements)}')\n",
    "\n",
    "        if len(measurements)>0:\n",
    "        # for n, measurements in enumerate(dets):\n",
    "            try: \n",
    "                hypotheses = data_associator.associate([track],\n",
    "                                                    measurements,\n",
    "                                                    this_time)\n",
    "                hypothesis = hypotheses[track]\n",
    "            \n",
    "                if hypothesis.measurement:\n",
    "                    post = updater.update(hypothesis)\n",
    "                    track.append(post)\n",
    "                else:  # When data associator says no detections are good enough, we'll keep the prediction\n",
    "                    track.append(hypothesis.prediction)\n",
    "\n",
    "                # print(f'{this_time}: {len(measurements)}: SUCCESS')\n",
    "\n",
    "            except:\n",
    "                # print(f'{this_time}: {len(measurements)}: ERROR')\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the ground truth and measurements with clutter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = rdp_matched['timestamp'].min()\n",
    "end_time = rdp_matched['timestamp'].max()\n",
    "meas_cart = CSVClutterReaderXY(matched_csv)\n",
    "adsb = ADSBTruthReader.multiple_ground_truth_reader([adsb_file])\n",
    "\n",
    "plot_all(all_measurements, adsb, start_time, end_time, tracks=[track], plot_type='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polar Coordinates\n",
    "Trying again, but changing the process to read rho and theta and, maybe later also radial velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Measurements - from Polar\n",
    "Recreate the all_measurements collection using the polar version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dets = [next(iter(detection[1])) for detection in matched_polar.detections_gen()]\n",
    "cluts = [next(iter(detection[1])) for detection in clutter.detections_gen()]\n",
    "\n",
    "# Combine detections with clutter\n",
    "all_measurements = dets + cluts\n",
    "all_measurements.sort(key=lambda obj: obj.timestamp)\n",
    "\n",
    "# Group objects by the second of their timestamp\n",
    "grouped_objects = defaultdict(set)\n",
    "\n",
    "for meas in all_measurements:\n",
    "# for meas in dets:\n",
    "    # Get the second part of the timestamp\n",
    "    timestamp_s = meas.timestamp.replace(microsecond=0)  # Truncate to second precision\n",
    "    grouped_objects[timestamp_s].add(meas)\n",
    "    # print(timestamp_s)\n",
    "\n",
    "# Convert grouped objects to a set of sets\n",
    "grouped_measurements = set(frozenset(group) for group in sorted(grouped_objects.values()))\n",
    "\n",
    "len(grouped_measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kalman Filtering Again...\n",
    "Should be the same from here forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition Model\n",
    "# q_const = 50\n",
    "q_x = q_const\n",
    "q_y = q_const\n",
    "transition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(q_x),\n",
    "                                                          ConstantVelocity(q_y)])\n",
    "\n",
    "# Create prior\n",
    "# predictor = KalmanPredictor(transition_model)\n",
    "# updater = KalmanUpdater(measurement_model)\n",
    "\n",
    "predictor = UnscentedKalmanPredictor(transition_model)\n",
    "updater = UnscentedKalmanUpdater(measurement_model)  # Keep alpha as default = 0.5\n",
    "\n",
    "hypothesiser = DistanceHypothesiser(predictor, updater, measure=Mahalanobis(), missed_distance=3)\n",
    "\n",
    "# Clear things out from prior runs\n",
    "hypothesis = None\n",
    "post = None\n",
    "if \"track\" in globals():\n",
    "    del(track)\n",
    "\n",
    "data_associator = NearestNeighbour(hypothesiser)\n",
    "\n",
    "# create a prior using the approximate start of the track\n",
    "prior = GaussianState([[track_start_x], [1], [track_start_y], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
    "\n",
    "# create a prior using the location of the radar\n",
    "# prior = GaussianState([[0], [q_const], [0], [q_const]], np.diag([default_variance, 0.5, default_variance, 0.5]), timestamp=start_time)\n",
    "\n",
    "# Loop through the predict, hypothesise, associate and update steps.\n",
    "track = Track([prior])\n",
    "for n, measurements in enumerate(grouped_list):\n",
    "    this_time = min(measurements, key=lambda meas: meas.timestamp).timestamp\n",
    "    this_time = this_time.replace(microsecond=0)\n",
    "    # print(f'{this_time}: {len(measurements)}')\n",
    "\n",
    "    if len(measurements)>0:\n",
    "        # print(f'{n}: {len(measurements)}')\n",
    "    # for n, measurements in enumerate(dets):\n",
    "        try: \n",
    "            hypotheses = data_associator.associate([track],\n",
    "                                                   measurements,\n",
    "                                                   this_time)\n",
    "            hypothesis = hypotheses[track]\n",
    "        \n",
    "            if hypothesis.measurement:\n",
    "                post = updater.update(hypothesis)\n",
    "                track.append(post)\n",
    "            else:  # When data associator says no detections are good enough, we'll keep the prediction\n",
    "                track.append(hypothesis.prediction)\n",
    "\n",
    "            # print(f'{this_time}: {len(measurements)}: SUCCESS')\n",
    "\n",
    "        except:\n",
    "            # print(f'{this_time}: {len(measurements)}: ERROR')\n",
    "            continue\n",
    "\n",
    "len(track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(all_measurements, adsb, start_time, end_time, tracks=track, plot_type='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very sensitive to the constant velocity parameter. They are also changing when re-running without making changes! I suspect the Kalman Filter is creating covariance matrices or something that are not getting reset when re-run.\n",
    "\n",
    "* 25: falls short of the ground truth consistently in the y- coordinate.\n",
    "* 35: does a pretty good job of matching the truth.\n",
    "* 50: matches well up until a point, then veers off track northerly for no apparent reason at"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asterix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
