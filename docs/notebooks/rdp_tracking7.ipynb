{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDP - Stone Soup Experiments\n",
    "[Documentation](https://stonesoup.readthedocs.io/en/v1.4/auto_examples/readers/Custom_Pandas_Dataloader.html#sphx-glr-auto-examples-readers-custom-pandas-dataloader-py)\n",
    "\n",
    "Continuing experiments - following tutorial #76: Probabilistic Data Association.\n",
    "\n",
    "For this tutorial, I need to extend the test data set from the previous notebook to add a second target. I'll look for a target of opportunity that is observed at the same time as the target considered so far. If possible, a few different cases would be interesting:\n",
    "1. A straight flight that does not interesect the first flight path\n",
    "1. A straight flight that DOES intersect the first flight path, but at a different time: 10610889\n",
    "1. A non-linear flight that does not cross the first flight\n",
    "1. A non-linear flight that DOES cross the first flight path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from importlib import reload  # Python 3.4+\n",
    "from typing import Tuple\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "from math import ceil\n",
    "from ordered_set import OrderedSet\n",
    "\n",
    "import dateutil\n",
    "from pymap3d import geodetic2enu\n",
    "\n",
    "import sys\n",
    "sys.path.append('C:/Users/ttrinter/git_repo/cspeed/data_common')\n",
    "sys.path.append('../../..')\n",
    "import data_functions as dfunc\n",
    "import visualizations as v\n",
    "from ttt_ss_funcs import generate_timestamps, ADSBTruthReader, CSVReaderXY, CSVReaderPolar, plot_all, CSVClutterReaderXY, group_plots\n",
    "\n",
    "\n",
    "from stonesoup.reader import DetectionReader, GroundTruthReader\n",
    "from stonesoup.base import Property\n",
    "from stonesoup.types.detection import Detection, Clutter\n",
    "from stonesoup.plotter import AnimatedPlotterly, Plotter\n",
    "\n",
    "from stonesoup.base import Property\n",
    "from stonesoup.buffered_generator import BufferedGenerator\n",
    "from stonesoup.functions import cart2sphere, sphere2cart\n",
    "from stonesoup.models.measurement.linear import LinearGaussian\n",
    "from stonesoup.models.measurement.nonlinear import CartesianToBearingRange\n",
    "from stonesoup.types.angle import Bearing\n",
    "from stonesoup.types.detection import Detection\n",
    "from stonesoup.types.groundtruth import GroundTruthState, GroundTruthPath\n",
    "\n",
    "# Tracker Imports\n",
    "from stonesoup.types.state import GaussianState\n",
    "\n",
    "plot_type = 'static' # or 'animated'\n",
    "\n",
    "sensor_positions = { 'RDU103': (51.52126391, 5.85862734)}\n",
    "\n",
    "METERS_in_NM = 1852\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/Read  to/from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from math import pi\n",
    "data_dir = 'C:/Users/ttrinter/git_repo/Stone-Soup/data'\n",
    "\n",
    "# Now there are 2 ADSB Files\n",
    "adsb_data = pd.DataFrame()\n",
    "adsb_files = [f'{data_dir}/adsb_straight.csv', f'{data_dir}/adsb_straight2.csv']\n",
    "for file in adsb_files:\n",
    "    this_data = pd.read_csv(file)\n",
    "    this_data['timestamp'] = pd.to_datetime(this_data['timestamp'], errors='coerce')\n",
    "    this_data = this_data.loc[~this_data['timestamp'].isna()]\n",
    "    this_data['timestamp'] = pd.to_datetime(this_data['timestamp'], errors='coerce')\n",
    "    this_data['timestamp'] = this_data['timestamp'].dt.tz_localize(None)\n",
    "\n",
    "    adsb_data = pd.concat([adsb_data, this_data])\n",
    "\n",
    "rdp_file = f'{data_dir}/rdp_straight.csv'\n",
    "# rdp_straight['timestamp'] = pd.to_datetime(rdp_straight['timestamp'], errors='coerce')\n",
    "# rdp_straight['theta_rad'] = np.deg2rad(rdp_straight.theta)\n",
    "# rdp_straight.loc[rdp_straight.theta_rad>2*pi, 'theta_rad'] = rdp_straight.loc[rdp_straight.theta_rad>2*pi, 'theta_rad'] - 2*pi \n",
    "\n",
    "# rdp_straight = rdp_straight.loc[~rdp_straight['timestamp'].isna()]\n",
    "# rdp_straight.to_csv(rdp_file, index=False)\n",
    "rdp_data = pd.read_csv(rdp_file)\n",
    "rdp_data['timestamp'] = pd.to_datetime(rdp_data['timestamp'], errors='coerce')\n",
    "rdp_data['timestamp'] = rdp_data['timestamp'].dt.tz_localize(None)\n",
    "\n",
    "\n",
    "# Matched Plots\n",
    "matched_csv1 = f'{data_dir}/rdp_matched.csv'\n",
    "matched_csv2 = f'{data_dir}/rdp_matched2.csv'\n",
    "rdp_matched = pd.read_csv(matched_csv1)\n",
    "rdp_matched['timestamp'] = pd.to_datetime(rdp_matched['timestamp'], errors='coerce')\n",
    "rdp_matched['timestamp'] = rdp_matched['timestamp'].dt.tz_localize(None)\n",
    "\n",
    "start_time = rdp_matched['timestamp'].min()\n",
    "end_time = rdp_matched['timestamp'].max()\n",
    "\n",
    "\n",
    "clutter_filename = f'{data_dir}/sample_clutter.csv'\n",
    "clutter_data = pd.read_csv(clutter_filename)\n",
    "\n",
    "print(f'ADSB: {len(adsb_data)}')\n",
    "print(f'RDP: {len(rdp_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.scatter_targets(clutter_data)\n",
    "plt.suptitle('Clutter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matched Data Set\n",
    "To make things even simpler, I'll grab the set of matched data for this test plane. Then most of the plots should be \"true\" detections. Let's see how the tracker does with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_address1 = 10537421 # plane #1\n",
    "target_address2 = 10610889 # plane #\n",
    "target_addresses = [target_address1, target_address2]\n",
    "file_dir = 'C:/Users/ttrinter/OneDrive - cspeed.com (1)/Documents/Data/Travis/2024-07-17'\n",
    "matched_file = '20240717_Travis_matched_rdp_61.xlsx'\n",
    "all_matched_data = pd.read_excel(f'{file_dir}/{matched_file}')\n",
    "matched_data = all_matched_data.loc[(all_matched_data.target_address.isin(target_addresses)) &\n",
    "                                (all_matched_data.close_enough==True) & \n",
    "                                (all_matched_data.timestamp_adsb>=start_time) & \n",
    "                                (all_matched_data.timestamp_adsb<=end_time)]\n",
    "matched_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_plot = v.plot_target_match2(matching=matched_data, \n",
    "                                    target_address=target_address1, \n",
    "                                    plot_show=True, \n",
    "                                    pd_loc='title')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_plot = v.plot_target_match2(matching=matched_data, \n",
    "                                    target_address=target_address2, \n",
    "                                    plot_show=True, \n",
    "                                    pd_loc='title')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Group by the 'category' column and plot each group separately\n",
    "for tgt, group in rdp_matched.groupby('target_address'):\n",
    "    ax.scatter(group['x'], group['y'], label=tgt, marker=\"+\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Matched RDP Plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdp_matched['x_m'] = rdp_matched.x * METERS_in_NM\n",
    "rdp_matched['y_m'] = rdp_matched.y * METERS_in_NM\n",
    "rdp_matched.sort_values('timestamp', inplace=True)\n",
    "\n",
    "# Track-starts:\n",
    "track_start1_t = rdp_matched.loc[rdp_matched.target_address==target_address1].iloc[0]['timestamp']\n",
    "track_start1_x = rdp_matched.loc[rdp_matched.target_address==target_address1].iloc[0]['x_m']\n",
    "track_start1_y = rdp_matched.loc[rdp_matched.target_address==target_address1].iloc[0]['y_m']\n",
    "\n",
    "# track_start2_t = rdp_matched.loc[rdp_matched.target_address==target_address2].iloc[0]['timestamp']\n",
    "# track_start2_x = rdp_matched.loc[rdp_matched.target_address==target_address2].iloc[0]['x_m']\n",
    "# track_start2_y = rdp_matched.loc[rdp_matched.target_address==target_address2].iloc[0]['y_m']\n",
    "\n",
    "rdp_matched[['target_address','timestamp', 'rho','theta','x_m','y_m']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adsb = ADSBTruthReader.multiple_ground_truth_reader(adsb_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = Plotter()\n",
    "plotter.plot_ground_truths(adsb, \n",
    "                        mapping=[0, 2], \n",
    "                        markersize = 5,\n",
    "                        marker = 's', \n",
    "                        markerfacecolor = 'none', \n",
    "                        alpha = 0.2)\n",
    "plt.grid()\n",
    "plt.title(\"Two Targets of Opportunity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detections\n",
    "matched_xy = CSVReaderXY(matched_csv1)\n",
    "matched_polar = CSVReaderPolar(matched_csv1)\n",
    "\n",
    "# Cutter\n",
    "clutter = CSVClutterReaderXY(clutter_filename)\n",
    "\n",
    "dets = [next(iter(detection[1])) for detection in matched_xy.detections_gen()]\n",
    "cluts = [next(iter(detection[1])) for detection in clutter.detections_gen()]\n",
    "\n",
    "# Combine detections with clutter\n",
    "all_measurements = dets + cluts\n",
    "all_measurements.sort(key=lambda obj: obj.timestamp)\n",
    "\n",
    "plot_all(start_time, \n",
    "         end_time,\n",
    "         all_measurements=all_measurements, \n",
    "        #  adsb=adsb, \n",
    "         plot_type='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Tutorial #7\n",
    "This is more or less the sam as tutorial 5, but changing out the Hypothesizer for a PDA hypothesizer. I'm using tutorial 6, rather than 5 to see how it handles the multiple tracks. Maybe it will resolve the weird, extended track issue from #6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of detection\n",
    "For the first time we introduce the possibility that, at any time-step, our sensor receives no detection from the target (i.e. $p_d < 1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stonesoup.models.transition.linear import CombinedLinearGaussianTransitionModel, \\\n",
    "                                               ConstantVelocity\n",
    "from stonesoup.predictor.kalman import KalmanPredictor\n",
    "from stonesoup.updater.kalman import KalmanUpdater\n",
    "from stonesoup.predictor.kalman import UnscentedKalmanPredictor\n",
    "from stonesoup.updater.kalman import UnscentedKalmanUpdater\n",
    "\n",
    "from stonesoup.types.track import Track\n",
    "from stonesoup.measures import Mahalanobis, Euclidean\n",
    "\n",
    "from stonesoup.types.array import StateVectors  # For storing state vectors during association\n",
    "from stonesoup.functions import gm_reduce_single  # For merging states to get posterior estimate\n",
    "from stonesoup.types.update import GaussianStateUpdate  # To store posterior estimate\n",
    "\n",
    "from stonesoup.hypothesiser.probability import PDAHypothesiser\n",
    "from stonesoup.dataassociator.probability import PDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measurement Model\n",
    "np.random.seed(42)\n",
    "default_variance = 50\n",
    "\n",
    "measurement_model = LinearGaussian(\n",
    "    ndim_state=4,   # Number of state dimensions (position and velocity in 2D)\n",
    "    mapping=(0, 2), # Mapping measurement vector index to state index\n",
    "    noise_covar=np.array([[default_variance, 0 ],  \n",
    "                          [0, default_variance]])\n",
    "    )  #Covariance matrix for Gaussian PDF\n",
    "\n",
    "# Transition Model\n",
    "q_const = 40\n",
    "q_x = q_const\n",
    "q_y = q_const\n",
    "transition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(q_x),\n",
    "                                                          ConstantVelocity(q_y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor = KalmanPredictor(transition_model)\n",
    "# updater = KalmanUpdater(measurement_model)\n",
    "\n",
    "predictor = UnscentedKalmanPredictor(transition_model)\n",
    "updater = UnscentedKalmanUpdater(measurement_model)  # Keep alpha as default = 0.5\n",
    "\n",
    "prob_det = 0.9\n",
    "hypothesiser = PDAHypothesiser(predictor=predictor,\n",
    "                               updater=updater,\n",
    "                               clutter_spatial_density=0.1,\n",
    "                               prob_detect=prob_det)\n",
    "\n",
    "data_associator = PDA(hypothesiser=hypothesiser)\n",
    "\n",
    "# Clear things out from prior runs\n",
    "hypothesis = None\n",
    "post = None\n",
    "\n",
    "prior1 = GaussianState([[track_start1_x], [1], [track_start1_y], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
    "# prior2 = GaussianState([[track_start2_x], [1], [track_start2_y], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
    "\n",
    "# create a prior using the location of the radar\n",
    "# prior = GaussianState([[0], [q_const], [0], [q_const]], np.diag([default_variance, 0.5, default_variance, 0.5]), timestamp=start_time)\n",
    "\n",
    "# Loop through the predict, hypothesise, associate and update steps.\n",
    "# track = Track([prior1])\n",
    "if \"tracks\" in globals():\n",
    "    del tracks\n",
    "\n",
    "if \"track\" in globals():\n",
    "    del(track)\n",
    "\n",
    "# tracks = {Track([prior1]), Track([prior2])}\n",
    "# tracks = {Track([prior1])}\n",
    "track = Track([prior1])\n",
    "\n",
    "grouped_sec, grouped_pass = group_plots(all_measurements)\n",
    "for n, measurements in enumerate(grouped_sec):\n",
    "    this_time = min(measurements, key=lambda meas: meas.timestamp).timestamp\n",
    "    this_time = this_time.replace(microsecond=0)\n",
    "\n",
    "    hypotheses = data_associator.associate({track},\n",
    "                                           measurements,\n",
    "                                           this_time)\n",
    "\n",
    "    hypotheses = hypotheses[track]\n",
    "\n",
    "    # Loop through each hypothesis, creating posterior states for each, and merge to calculate\n",
    "    # approximation to actual posterior state mean and covariance.\n",
    "    posterior_states = []\n",
    "    posterior_state_weights = []\n",
    "    for hypothesis in hypotheses:\n",
    "        if not hypothesis:\n",
    "            posterior_states.append(hypothesis.prediction)\n",
    "        else:\n",
    "            posterior_state = updater.update(hypothesis)\n",
    "            posterior_states.append(posterior_state)\n",
    "        posterior_state_weights.append(\n",
    "            hypothesis.probability)\n",
    "\n",
    "    means = StateVectors([state.state_vector for state in posterior_states])\n",
    "    covars = np.stack([state.covar for state in posterior_states], axis=2)\n",
    "    weights = np.asarray(posterior_state_weights)\n",
    "\n",
    "    # Reduce mixture of states to one posterior estimate Gaussian.\n",
    "    post_mean, post_covar = gm_reduce_single(means, covars, weights)\n",
    "\n",
    "    # Add a Gaussian state approximation to the track.\n",
    "    track.append(GaussianStateUpdate(\n",
    "        post_mean, post_covar,\n",
    "        hypotheses,\n",
    "        hypotheses[0].measurement.timestamp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the ground truth and measurements with clutter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(start_time,\n",
    "         end_time,\n",
    "         all_measurements=all_measurements, \n",
    "        #  adsb=adsb,\n",
    "         tracks=track,\n",
    "         plot_type='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polar Coordinates\n",
    "Trying again, but changing the process to read rho and theta and, maybe later also radial velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_type='static'\n",
    "# rdp = RDPReader(rdp_file,\n",
    "start_time = rdp_matched['timestamp'].min()\n",
    "end_time = rdp_matched['timestamp'].max()\n",
    "timestamps = generate_timestamps(start_time, end_time)\n",
    "\n",
    "# Detections\n",
    "meas_polar = CSVReaderPolar(matched_csv2)\n",
    "\n",
    "dets = [next(iter(detection[1])) for detection in meas_polar.detections_gen()]\n",
    "cluts = [next(iter(detection[1])) for detection in clutter.detections_gen()]\n",
    "\n",
    "# Combine detections with clutter\n",
    "all_measurements = dets + cluts\n",
    "all_measurements.sort(key=lambda obj: obj.timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kalman Filtering Again...\n",
    "Should be the same from here forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measurement_model = CartesianToBearingRange(ndim_state=4, \n",
    "#                                             mapping=(0,2), \n",
    "#                                             noise_covar=np.array([[default_variance, 0 ],\n",
    "#                                                                   [0, default_variance]]))\n",
    "\n",
    "predictor = UnscentedKalmanPredictor(transition_model)\n",
    "updater = UnscentedKalmanUpdater(measurement_model)  # Keep alpha as default = 0.5\n",
    "\n",
    "prob_det = 0.9\n",
    "hypothesiser = PDAHypothesiser(predictor=predictor,\n",
    "                               updater=updater,\n",
    "                               clutter_spatial_density=0.125,\n",
    "                               prob_detect=prob_det)\n",
    "\n",
    "data_associator = PDA(hypothesiser=hypothesiser)\n",
    "\n",
    "# Clear things out from prior runs\n",
    "hypothesis = None\n",
    "post = None\n",
    "\n",
    "# create a prior using the approximate start of the track\n",
    "# prior = GaussianState([[track_start_x], [1], [track_start_y], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
    "\n",
    "prior1 = GaussianState([[track_start1_x], [1], [track_start1_y], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
    "# prior2 = GaussianState([[track_start2_x], [1], [track_start2_y], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)\n",
    "\n",
    "# create a prior using the location of the radar\n",
    "# prior = GaussianState([[0], [q_const], [0], [q_const]], np.diag([default_variance, 0.5, default_variance, 0.5]), timestamp=start_time)\n",
    "\n",
    "# Loop through the predict, hypothesise, associate and update steps.\n",
    "# del(tracks)\n",
    "if \"tracks\" in globals():\n",
    "    del tracks\n",
    "\n",
    "# tracks = {Track([prior1]), Track([prior2])}\n",
    "tracks = {Track([prior1])}\n",
    "track = Track([prior1])\n",
    "\n",
    "grouped_sec, grouped_pass = group_plots(all_measurements)\n",
    "for n, measurements in enumerate(grouped_pass):\n",
    "    this_time = min(measurements, key=lambda meas: meas.timestamp).timestamp\n",
    "    this_time = this_time.replace(microsecond=0)\n",
    "    # print(this_time)\n",
    "\n",
    "    # # Calculate all hypothesis pairs and associate the elements in the best subset to the tracks.\n",
    "    # if len(measurements)>0:\n",
    "    # # for n, measurements in enumerate(dets):\n",
    "    #     # try: \n",
    "    #     hypotheses = data_associator.associate(tracks,\n",
    "    #                                             measurements,\n",
    "    #                                             this_time)\n",
    "    #     for track in tracks:\n",
    "    #         hypothesis = hypotheses[track]\n",
    "    #         if hypothesis.measurement:\n",
    "    #             post = updater.update(hypothesis)\n",
    "    #             track.append(post)\n",
    "    #             print('track updated')\n",
    "    #         else:  # When data associator says no detections are good enough, we'll keep the prediction\n",
    "    #             track.append(hypothesis.prediction)\n",
    "    #             print('no update')\n",
    "\n",
    "    #     # except:\n",
    "    #     #     print(f'{this_time}: {len(measurements)}: ERROR')\n",
    "    #     #     continue\n",
    "\n",
    "    try:\n",
    "        hypotheses = data_associator.associate({track},\n",
    "                                            measurements,\n",
    "                                            this_time)\n",
    "\n",
    "        hypotheses = hypotheses[track]\n",
    "\n",
    "        # Loop through each hypothesis, creating posterior states for each, and merge to calculate\n",
    "        # approximation to actual posterior state mean and covariance.\n",
    "        posterior_states = []\n",
    "        posterior_state_weights = []\n",
    "        for hypothesis in hypotheses:\n",
    "            if not hypothesis:\n",
    "                posterior_states.append(hypothesis.prediction)\n",
    "            else:\n",
    "                posterior_state = updater.update(hypothesis)\n",
    "                posterior_states.append(posterior_state)\n",
    "            posterior_state_weights.append(\n",
    "                hypothesis.probability)\n",
    "\n",
    "        means = StateVectors([state.state_vector for state in posterior_states])\n",
    "        covars = np.stack([state.covar for state in posterior_states], axis=2)\n",
    "        weights = np.asarray(posterior_state_weights)\n",
    "\n",
    "        # Reduce mixture of states to one posterior estimate Gaussian.\n",
    "        post_mean, post_covar = gm_reduce_single(means, covars, weights)\n",
    "\n",
    "        # Add a Gaussian state approximation to the track.\n",
    "        track.append(GaussianStateUpdate(\n",
    "            post_mean, post_covar,\n",
    "            hypotheses,\n",
    "            hypotheses[0].measurement.timestamp))\n",
    "        print(f'Track updated {this_time}')\n",
    "        \n",
    "    except:\n",
    "        print(f'Error at {this_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(start_time,\n",
    "         end_time,\n",
    "         all_measurements=all_measurements, \n",
    "         adsb=adsb,\n",
    "         tracks=track,\n",
    "         plot_type='static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cspeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
